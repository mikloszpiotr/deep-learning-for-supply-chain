{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f32e341-2a77-4471-90e1-fd5cf4728e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inventory Optimization with Deep Reinforcement Learning (TensorFlow/Keras)\n",
    "=========================================================================\n",
    "Goal:\n",
    "- Train an RL agent to decide \"how much to order today\" to minimize total inventory cost\n",
    "  under stochastic demand and lead time uncertainty.\n",
    "\n",
    "What this file includes:\n",
    "1) A small, realistic single-item inventory simulator (custom environment)\n",
    "2) A Deep Q-Network (DQN) agent in TensorFlow/Keras\n",
    "3) Training loop + evaluation vs. a classic (s, S) reorder policy baseline\n",
    "\n",
    "Typical supply chain costs modeled:\n",
    "- Holding cost (per unit per day)\n",
    "- Stockout / backorder penalty (per unit short)\n",
    "- Ordering cost (variable per unit + optional fixed cost per order)\n",
    "\n",
    "State (what the agent sees):\n",
    "- On-hand inventory\n",
    "- Pipeline inventory by lead-time remaining (vector)\n",
    "- Current day-of-week (optional seasonality hook)\n",
    "\n",
    "Action (what the agent controls):\n",
    "- Order quantity chosen from a discrete set (e.g., {0, 5, 10, ..., 50})\n",
    "\n",
    "Business interpretation:\n",
    "- The agent learns a dynamic reorder point / safety stock behavior that adapts\n",
    "  to pipeline and upcoming demand patterns, instead of a fixed static rule.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "from typing import Deque, Tuple, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee7186a-1e81-4b6d-a958-256de9f19493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 0) Reproducibility + global config\n",
    "# =============================================================================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Training horizon settings\n",
    "EPISODES = 500\n",
    "DAYS_PER_EPISODE = 180  # simulate 180 days per episode (roughly half-year)\n",
    "\n",
    "# DQN hyperparameters\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "REPLAY_CAPACITY = 50_000\n",
    "LEARN_START = 2_000        # wait until replay has enough transitions\n",
    "TRAIN_EVERY = 1            # gradient steps frequency (in environment steps)\n",
    "TARGET_UPDATE_EVERY = 1_000\n",
    "\n",
    "# Exploration schedule (epsilon-greedy)\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY_STEPS = 50_000   # anneal epsilon over these steps\n",
    "\n",
    "# Action space: discrete order quantities\n",
    "MAX_ORDER_QTY = 50\n",
    "ORDER_STEP = 5\n",
    "ACTIONS = np.arange(0, MAX_ORDER_QTY + 1, ORDER_STEP)  # e.g., [0,5,10,...,50]\n",
    "N_ACTIONS = len(ACTIONS)\n",
    "\n",
    "# Environment settings (can be tuned to match your case study)\n",
    "MAX_LEAD_TIME = 5  # maximum possible lead time (days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75335fd0-be49-4d55-b523-361be6922e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1) Inventory simulator environment (single-item, stochastic demand, lead time)\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class InventoryEnvConfig:\n",
    "    # Demand process\n",
    "    mean_demand: float = 20.0\n",
    "    demand_std: float = 5.0          # for normal demand (clipped at >=0)\n",
    "    use_weekly_seasonality: bool = True\n",
    "\n",
    "    # Lead time (stochastic). You can replace with empirical distribution.\n",
    "    min_lead_time: int = 1\n",
    "    max_lead_time: int = MAX_LEAD_TIME\n",
    "\n",
    "    # Cost parameters\n",
    "    holding_cost: float = 1.0        # cost per unit held per day\n",
    "    stockout_cost: float = 10.0      # penalty per unit short per day (lost sales/backorder)\n",
    "    unit_order_cost: float = 2.0     # variable ordering cost per unit\n",
    "    fixed_order_cost: float = 20.0   # fixed cost if you order > 0 (set 0 if not needed)\n",
    "\n",
    "    # Inventory bounds (for normalization / clipping)\n",
    "    max_inventory: int = 300         # on-hand upper bound (soft cap via clipping)\n",
    "    max_pipeline_per_bucket: int = 200  # pipeline bucket upper bound (soft cap)\n",
    "\n",
    "\n",
    "class InventoryEnv:\n",
    "    \"\"\"\n",
    "    A lightweight inventory control environment designed for DQN.\n",
    "\n",
    "    Dynamics per day t:\n",
    "    1) Receive deliveries arriving today\n",
    "    2) Observe state\n",
    "    3) Choose action: order quantity q\n",
    "    4) Sample lead time L, put q into pipeline arriving at t+L\n",
    "    5) Sample demand D_t, fulfill from on-hand\n",
    "    6) Compute costs and reward = - total_cost\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: InventoryEnvConfig, horizon_days: int, seed: int = 0):\n",
    "        self.cfg = config\n",
    "        self.horizon_days = horizon_days\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Internal state\n",
    "        self.day: int = 0\n",
    "        self.on_hand: int = 0\n",
    "\n",
    "        # Pipeline is a vector of length max_lead_time, where:\n",
    "        # pipeline[i] = units arriving in (i+1) days\n",
    "        self.pipeline: np.ndarray = np.zeros(self.cfg.max_lead_time, dtype=np.int32)\n",
    "\n",
    "        # For logging\n",
    "        self.total_cost: float = 0.0\n",
    "\n",
    "    @property\n",
    "    def state_dim(self) -> int:\n",
    "        # on_hand (1) + pipeline buckets (max_lead_time) + day_of_week one scalar (1)\n",
    "        return 1 + self.cfg.max_lead_time + 1\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.day = 0\n",
    "        self.total_cost = 0.0\n",
    "\n",
    "        # Initialize with some starting inventory/pipeline (optional)\n",
    "        self.on_hand = int(self.cfg.mean_demand * 2)  # start with ~2 days cover\n",
    "        self.pipeline = np.zeros(self.cfg.max_lead_time, dtype=np.int32)\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action_index: int) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        action_index: index into ACTIONS array (discrete order quantities)\n",
    "        returns: obs_next, reward, done, info\n",
    "        \"\"\"\n",
    "        assert 0 <= action_index < N_ACTIONS\n",
    "        order_qty = int(ACTIONS[action_index])\n",
    "\n",
    "        # --- 1) Receive deliveries arriving today (pipeline bucket 0 means arriving in 1 day,\n",
    "        # so \"arriving today\" is not stored there. We implement shift:\n",
    "        # pipeline[0] is arriving tomorrow; so first shift pipeline, but arrivals today are tracked separately.\n",
    "        arrivals_today = 0  # this env stores arrivals only via pipeline shift; so keep 0 here.\n",
    "        # If you prefer \"pipeline[0] arrives today\", adjust indexing accordingly.\n",
    "\n",
    "        # Shift pipeline: items in bucket 0 arrive now? We'll define:\n",
    "        # pipeline[0] arrives TODAY, pipeline[1] arrives in 1 day, ...\n",
    "        # That tends to be simpler for daily step ordering.\n",
    "        arrivals_today = int(self.pipeline[0])\n",
    "        self.on_hand += arrivals_today\n",
    "\n",
    "        # Move the pipeline forward by one day\n",
    "        self.pipeline[:-1] = self.pipeline[1:]\n",
    "        self.pipeline[-1] = 0\n",
    "\n",
    "        # --- 2) Observe state (agent sees after arrivals, before ordering)\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # --- 3) Place new order into pipeline with stochastic lead time\n",
    "        if order_qty > 0:\n",
    "            lead_time = int(self.rng.integers(self.cfg.min_lead_time, self.cfg.max_lead_time + 1))\n",
    "            # If lead_time=1, it arrives tomorrow => bucket index (lead_time-1)\n",
    "            bucket = lead_time - 1\n",
    "            self.pipeline[bucket] += order_qty\n",
    "        else:\n",
    "            lead_time = 0  # no order\n",
    "\n",
    "        # --- 4) Sample demand and fulfill\n",
    "        demand = self._sample_demand(self.day)\n",
    "        demand = int(max(0, demand))\n",
    "\n",
    "        fulfilled = min(self.on_hand, demand)\n",
    "        self.on_hand -= fulfilled\n",
    "        short = demand - fulfilled  # unmet demand (lost sales/backorder proxy)\n",
    "\n",
    "        # --- 5) Compute costs\n",
    "        holding_cost = self.cfg.holding_cost * self.on_hand\n",
    "        stockout_cost = self.cfg.stockout_cost * short\n",
    "\n",
    "        variable_order_cost = self.cfg.unit_order_cost * order_qty\n",
    "        fixed_order_cost = self.cfg.fixed_order_cost if order_qty > 0 else 0.0\n",
    "\n",
    "        total_cost_today = holding_cost + stockout_cost + variable_order_cost + fixed_order_cost\n",
    "        self.total_cost += total_cost_today\n",
    "\n",
    "        # Reward is negative cost (RL maximizes reward)\n",
    "        reward = -float(total_cost_today)\n",
    "\n",
    "        # --- 6) Next day\n",
    "        self.day += 1\n",
    "        done = self.day >= self.horizon_days\n",
    "\n",
    "        obs_next = self._get_obs()\n",
    "\n",
    "        info = {\n",
    "            \"day\": self.day,\n",
    "            \"demand\": demand,\n",
    "            \"fulfilled\": fulfilled,\n",
    "            \"short\": short,\n",
    "            \"arrivals_today\": arrivals_today,\n",
    "            \"order_qty\": order_qty,\n",
    "            \"lead_time\": lead_time,\n",
    "            \"on_hand\": int(self.on_hand),\n",
    "            \"pipeline\": self.pipeline.copy(),\n",
    "            \"cost_today\": float(total_cost_today),\n",
    "            \"total_cost\": float(self.total_cost),\n",
    "        }\n",
    "        return obs_next, reward, done, info\n",
    "\n",
    "    def _sample_demand(self, day: int) -> int:\n",
    "        \"\"\"\n",
    "        Demand model:\n",
    "        - Base normal distribution around mean_demand\n",
    "        - Optional weekly seasonality factor (Mon..Sun)\n",
    "        Replace this with your real demand generator or historical bootstrap.\n",
    "        \"\"\"\n",
    "        base = self.rng.normal(self.cfg.mean_demand, self.cfg.demand_std)\n",
    "\n",
    "        if not self.cfg.use_weekly_seasonality:\n",
    "            return int(round(base))\n",
    "\n",
    "        # Simple weekly pattern: e.g., weekend higher/lower (customize as needed)\n",
    "        dow = day % 7  # 0..6\n",
    "        seasonal_multiplier = [1.0, 1.0, 1.05, 1.05, 1.10, 1.15, 1.10][dow]\n",
    "        return int(round(base * seasonal_multiplier))\n",
    "\n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Observation as a normalized float vector:\n",
    "        [on_hand, pipeline_0, ..., pipeline_{L-1}, day_of_week]\n",
    "        \"\"\"\n",
    "        on_hand = np.clip(self.on_hand, 0, self.cfg.max_inventory)\n",
    "        pipe = np.clip(self.pipeline, 0, self.cfg.max_pipeline_per_bucket)\n",
    "\n",
    "        dow = self.day % 7\n",
    "\n",
    "        obs = np.concatenate([\n",
    "            np.array([on_hand], dtype=np.float32),\n",
    "            pipe.astype(np.float32),\n",
    "            np.array([dow], dtype=np.float32),\n",
    "        ])\n",
    "\n",
    "        # Normalize for NN stability\n",
    "        obs[0] /= float(self.cfg.max_inventory)\n",
    "        obs[1:1 + self.cfg.max_lead_time] /= float(self.cfg.max_pipeline_per_bucket)\n",
    "        obs[-1] /= 6.0  # day_of_week in [0,6] -> [0,1]\n",
    "\n",
    "        return obs.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5acc62-c863-454b-9a1d-8e81023f6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2) Replay Buffer for DQN\n",
    "# =============================================================================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer: Deque = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, s: np.ndarray, a: int, r: float, s2: np.ndarray, done: bool):\n",
    "        self.buffer.append((s, a, r, s2, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s2, d = map(np.array, zip(*batch))\n",
    "        return (\n",
    "            s.astype(np.float32),\n",
    "            a.astype(np.int32),\n",
    "            r.astype(np.float32),\n",
    "            s2.astype(np.float32),\n",
    "            d.astype(np.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b2aad05-a710-48af-a0a4-ae5242ce5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3) DQN Model (Q-network) in TensorFlow/Keras\n",
    "# =============================================================================\n",
    "def build_q_network(state_dim: int, n_actions: int) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Q(s,a) approximator:\n",
    "    - Input: state vector\n",
    "    - Output: Q-values for each discrete action\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(state_dim,), name=\"state\")\n",
    "\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(inputs)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "    # Output layer: one Q-value per action\n",
    "    q_values = tf.keras.layers.Dense(n_actions, activation=None, name=\"q_values\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=q_values)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4287cbf5-654c-43b6-a2ef-82e064535697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4) DQN Agent (epsilon-greedy + target network)\n",
    "# =============================================================================\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim: int, n_actions: int):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Online network (trainable) and target network (stable bootstrap target)\n",
    "        self.q_online = build_q_network(state_dim, n_actions)\n",
    "        self.q_target = build_q_network(state_dim, n_actions)\n",
    "        self.q_target.set_weights(self.q_online.get_weights())\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "        # Step counter for epsilon schedule + target updates\n",
    "        self.global_step = 0\n",
    "\n",
    "    def epsilon(self) -> float:\n",
    "        \"\"\"\n",
    "        Linear decay: EPS_START -> EPS_END over EPS_DECAY_STEPS\n",
    "        \"\"\"\n",
    "        frac = min(1.0, self.global_step / float(EPS_DECAY_STEPS))\n",
    "        return EPS_START + frac * (EPS_END - EPS_START)\n",
    "\n",
    "    def act(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Epsilon-greedy action selection.\n",
    "        \"\"\"\n",
    "        eps = self.epsilon()\n",
    "        self.global_step += 1\n",
    "\n",
    "        if random.random() < eps:\n",
    "            return random.randrange(self.n_actions)\n",
    "\n",
    "        # Greedy action from Q-network\n",
    "        s = state.reshape(1, -1).astype(np.float32)\n",
    "        q = self.q_online(s, training=False).numpy()[0]\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "    @tf.function\n",
    "    def _train_step(self, s, a, r, s2, done):\n",
    "        \"\"\"\n",
    "        One gradient update using DQN target:\n",
    "        y = r + gamma * (1-done) * max_a' Q_target(s2, a')\n",
    "        loss = MSE(Q_online(s, a), y)\n",
    "        \"\"\"\n",
    "        # Compute target y\n",
    "        q_next = self.q_target(s2, training=False)             # [B, A]\n",
    "        max_q_next = tf.reduce_max(q_next, axis=1)             # [B]\n",
    "        y = r + GAMMA * (1.0 - done) * max_q_next              # [B]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_all = self.q_online(s, training=True)            # [B, A]\n",
    "\n",
    "            # Gather Q(s,a) for taken actions\n",
    "            idx = tf.stack([tf.range(tf.shape(a)[0]), a], axis=1)\n",
    "            q_sa = tf.gather_nd(q_all, idx)                    # [B]\n",
    "\n",
    "            loss = tf.reduce_mean(tf.square(y - q_sa))\n",
    "\n",
    "        grads = tape.gradient(loss, self.q_online.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_online.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def learn(self, replay: ReplayBuffer) -> float:\n",
    "        \"\"\"\n",
    "        Sample a batch and update online network.\n",
    "        Also periodically update the target network weights.\n",
    "        \"\"\"\n",
    "        s, a, r, s2, done = replay.sample(BATCH_SIZE)\n",
    "\n",
    "        loss = self._train_step(\n",
    "            tf.convert_to_tensor(s),\n",
    "            tf.convert_to_tensor(a),\n",
    "            tf.convert_to_tensor(r),\n",
    "            tf.convert_to_tensor(s2),\n",
    "            tf.convert_to_tensor(done),\n",
    "        )\n",
    "\n",
    "        # Target network hard update\n",
    "        if (self.global_step % TARGET_UPDATE_EVERY) == 0:\n",
    "            self.q_target.set_weights(self.q_online.get_weights())\n",
    "\n",
    "        return float(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa249f4-624f-4a1e-a98d-cf72fbd27366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5) Baseline policy: classic (s, S) reorder rule (for comparison)\n",
    "# =============================================================================\n",
    "class ReorderPolicy_sS:\n",
    "    \"\"\"\n",
    "    If inventory_position <= s: order up to S\n",
    "    inventory_position = on_hand + pipeline_total\n",
    "    \"\"\"\n",
    "    def __init__(self, s: int, S: int):\n",
    "        assert S >= s\n",
    "        self.s = s\n",
    "        self.S = S\n",
    "\n",
    "    def act(self, on_hand: int, pipeline: np.ndarray) -> int:\n",
    "        inv_pos = on_hand + int(pipeline.sum())\n",
    "        if inv_pos <= self.s:\n",
    "            return max(0, self.S - inv_pos)\n",
    "        return 0\n",
    "\n",
    "\n",
    "def order_qty_to_action_index(order_qty: int) -> int:\n",
    "    \"\"\"\n",
    "    Convert a quantity to the nearest discrete action in ACTIONS.\n",
    "    \"\"\"\n",
    "    order_qty = int(np.clip(order_qty, ACTIONS[0], ACTIONS[-1]))\n",
    "    # snap to step\n",
    "    snapped = int(round(order_qty / ORDER_STEP) * ORDER_STEP)\n",
    "    snapped = int(np.clip(snapped, ACTIONS[0], ACTIONS[-1]))\n",
    "    return int(np.where(ACTIONS == snapped)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72af4cf0-344d-46b6-bc86-70aae83dd437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6) Training + Evaluation helpers\n",
    "# =============================================================================\n",
    "def run_episode_with_agent(env: InventoryEnv, agent: DQNAgent, replay: ReplayBuffer, training: bool) -> Dict:\n",
    "    \"\"\"\n",
    "    Runs 1 episode using DQN agent.\n",
    "    If training=True:\n",
    "      - store transitions\n",
    "      - train periodically\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "\n",
    "    total_reward = 0.0\n",
    "    total_loss = 0.0\n",
    "    n_updates = 0\n",
    "\n",
    "    while not done:\n",
    "        a = agent.act(s)\n",
    "        s2, r, done, info = env.step(a)\n",
    "\n",
    "        total_reward += r\n",
    "\n",
    "        if training:\n",
    "            replay.add(s, a, r, s2, done)\n",
    "\n",
    "            # Train only when replay has enough samples\n",
    "            if len(replay) >= LEARN_START and (agent.global_step % TRAIN_EVERY == 0):\n",
    "                loss = agent.learn(replay)\n",
    "                total_loss += loss\n",
    "                n_updates += 1\n",
    "\n",
    "        s = s2\n",
    "\n",
    "    return {\n",
    "        \"total_reward\": float(total_reward),\n",
    "        \"total_cost\": float(info[\"total_cost\"]),\n",
    "        \"avg_loss\": float(total_loss / max(1, n_updates)),\n",
    "        \"updates\": int(n_updates),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_episode_with_policy(env: InventoryEnv, policy: ReorderPolicy_sS) -> Dict:\n",
    "    \"\"\"\n",
    "    Runs 1 episode using a non-learning baseline policy.\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # De-normalize extraction helper:\n",
    "    # obs = [on_hand_norm, pipeline_norm..., dow_norm]\n",
    "    def denorm_on_hand(obs):\n",
    "        return int(round(obs[0] * env.cfg.max_inventory))\n",
    "\n",
    "    def denorm_pipeline(obs):\n",
    "        pipe_norm = obs[1:1 + env.cfg.max_lead_time]\n",
    "        pipe = (pipe_norm * env.cfg.max_pipeline_per_bucket).round().astype(int)\n",
    "        return pipe\n",
    "\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        on_hand = denorm_on_hand(s)\n",
    "        pipe = denorm_pipeline(s)\n",
    "\n",
    "        order_qty = policy.act(on_hand, pipe)\n",
    "        a = order_qty_to_action_index(order_qty)\n",
    "\n",
    "        s2, r, done, info = env.step(a)\n",
    "        total_reward += r\n",
    "        s = s2\n",
    "\n",
    "    return {\"total_reward\": float(total_reward), \"total_cost\": float(info[\"total_cost\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a47ace0-b1ee-4604-a8d9-947f1c26e36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim = 7, Actions = 11 ([0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50])\n",
      "Baseline (s,S) = (77, 157)  | safety_stock≈17, avg_lt≈3.00\n",
      "EP   25 | avg_cost(last25)=64845.08 | avg_loss=13507.69178 | eps=0.914\n",
      "EP   50 | avg_cost(last25)=44195.52 | avg_loss=32081.96900 | eps=0.829\n",
      "EP   75 | avg_cost(last25)=28632.84 | avg_loss=68666.94621 | eps=0.744\n",
      "EP  100 | avg_cost(last25)=22893.96 | avg_loss=90401.99663 | eps=0.658\n",
      "EP  125 | avg_cost(last25)=21435.76 | avg_loss=92433.20650 | eps=0.573\n",
      "EP  150 | avg_cost(last25)=20392.36 | avg_loss=100011.53665 | eps=0.487\n",
      "EP  175 | avg_cost(last25)=19173.68 | avg_loss=94663.01036 | eps=0.402\n",
      "EP  200 | avg_cost(last25)=19005.88 | avg_loss=88652.05983 | eps=0.316\n",
      "EP  225 | avg_cost(last25)=18573.40 | avg_loss=87852.41867 | eps=0.231\n",
      "EP  250 | avg_cost(last25)=18289.92 | avg_loss=84201.79950 | eps=0.145\n",
      "EP  275 | avg_cost(last25)=18762.28 | avg_loss=87433.49176 | eps=0.059\n",
      "EP  300 | avg_cost(last25)=18474.12 | avg_loss=74763.30404 | eps=0.050\n",
      "EP  325 | avg_cost(last25)=18625.88 | avg_loss=57404.79071 | eps=0.050\n",
      "EP  350 | avg_cost(last25)=19410.76 | avg_loss=50948.16041 | eps=0.050\n",
      "EP  375 | avg_cost(last25)=18922.24 | avg_loss=52692.23920 | eps=0.050\n",
      "EP  400 | avg_cost(last25)=18637.64 | avg_loss=57299.32750 | eps=0.050\n",
      "EP  425 | avg_cost(last25)=18761.88 | avg_loss=61413.27435 | eps=0.050\n",
      "EP  450 | avg_cost(last25)=19080.88 | avg_loss=64298.56196 | eps=0.050\n",
      "EP  475 | avg_cost(last25)=18851.16 | avg_loss=67742.49133 | eps=0.050\n",
      "EP  500 | avg_cost(last25)=19070.12 | avg_loss=71179.03730 | eps=0.050\n",
      "\n",
      "=== Evaluation Results (lower cost is better) ===\n",
      "DQN:      mean_cost=18449.76 | std=552.65\n",
      "Baseline: mean_cost=19358.82 | std=868.23\n",
      "Relative improvement vs baseline: 4.70%\n",
      "\n",
      "Sample episode costs:\n",
      "DQN:      [18806.0, 18325.0, 18320.0, 18254.0, 18477.0, 17750.0, 18325.0, 18658.0, 18928.0, 17706.0]\n",
      "Baseline: [18733.0, 21147.0, 19009.0, 18106.0, 19554.0, 19636.0, 19168.0, 18982.0, 19394.0, 19906.0]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7) Main: Train DQN and compare vs baseline\n",
    "# =============================================================================\n",
    "def main():\n",
    "    cfg = InventoryEnvConfig(\n",
    "        mean_demand=20.0,\n",
    "        demand_std=6.0,\n",
    "        use_weekly_seasonality=True,\n",
    "        min_lead_time=1,\n",
    "        max_lead_time=5,\n",
    "        holding_cost=1.0,\n",
    "        stockout_cost=12.0,\n",
    "        unit_order_cost=2.0,\n",
    "        fixed_order_cost=15.0,\n",
    "        max_inventory=300,\n",
    "        max_pipeline_per_bucket=200,\n",
    "    )\n",
    "\n",
    "    # Create environment\n",
    "    env = InventoryEnv(cfg, horizon_days=DAYS_PER_EPISODE, seed=SEED)\n",
    "    print(f\"State dim = {env.state_dim}, Actions = {N_ACTIONS} ({ACTIONS.tolist()})\")\n",
    "\n",
    "    # Create DQN agent and replay buffer\n",
    "    agent = DQNAgent(state_dim=env.state_dim, n_actions=N_ACTIONS)\n",
    "    replay = ReplayBuffer(capacity=REPLAY_CAPACITY)\n",
    "\n",
    "    # Baseline: simple reorder policy\n",
    "    # Rule-of-thumb starting point:\n",
    "    # - reorder point s ~ mean_demand*(avg_lead_time) + safety_stock\n",
    "    # - safety stock ~ z * sigma_demand * sqrt(lead_time)\n",
    "    avg_lt = (cfg.min_lead_time + cfg.max_lead_time) / 2.0\n",
    "    safety_stock = int(round(1.65 * cfg.demand_std * np.sqrt(avg_lt)))  # ~95% CSL-ish\n",
    "    s = int(round(cfg.mean_demand * avg_lt + safety_stock))\n",
    "    S = s + 80  # order-up-to level (tune)\n",
    "    baseline = ReorderPolicy_sS(s=s, S=S)\n",
    "    print(f\"Baseline (s,S) = ({s}, {S})  | safety_stock≈{safety_stock}, avg_lt≈{avg_lt:.2f}\")\n",
    "\n",
    "    # --- Training loop\n",
    "    history = []\n",
    "    for ep in range(1, EPISODES + 1):\n",
    "        metrics = run_episode_with_agent(env, agent, replay, training=True)\n",
    "        history.append(metrics)\n",
    "\n",
    "        # Lightweight logging every N episodes\n",
    "        if ep % 25 == 0:\n",
    "            recent = history[-25:]\n",
    "            avg_cost = np.mean([h[\"total_cost\"] for h in recent])\n",
    "            avg_loss = np.mean([h[\"avg_loss\"] for h in recent])\n",
    "            eps = agent.epsilon()\n",
    "            print(f\"EP {ep:4d} | avg_cost(last25)={avg_cost:8.2f} | avg_loss={avg_loss:8.5f} | eps={eps:5.3f}\")\n",
    "\n",
    "    # --- Evaluation: run a few episodes without exploration (set epsilon ~ 0)\n",
    "    # For a clean eval, we'll temporarily override epsilon by setting global_step large.\n",
    "    agent.global_step = int(EPS_DECAY_STEPS * 10)\n",
    "\n",
    "    eval_runs = 50\n",
    "    dqn_costs = []\n",
    "    base_costs = []\n",
    "\n",
    "    for _ in range(eval_runs):\n",
    "        # DQN (no training)\n",
    "        dqn_metrics = run_episode_with_agent(env, agent, replay, training=False)\n",
    "        dqn_costs.append(dqn_metrics[\"total_cost\"])\n",
    "\n",
    "        # Baseline\n",
    "        base_metrics = run_episode_with_policy(env, baseline)\n",
    "        base_costs.append(base_metrics[\"total_cost\"])\n",
    "\n",
    "    print(\"\\n=== Evaluation Results (lower cost is better) ===\")\n",
    "    print(f\"DQN:      mean_cost={np.mean(dqn_costs):.2f} | std={np.std(dqn_costs):.2f}\")\n",
    "    print(f\"Baseline: mean_cost={np.mean(base_costs):.2f} | std={np.std(base_costs):.2f}\")\n",
    "\n",
    "    improvement = (np.mean(base_costs) - np.mean(dqn_costs)) / np.mean(base_costs) * 100.0\n",
    "    print(f\"Relative improvement vs baseline: {improvement:.2f}%\")\n",
    "\n",
    "    # Optional: show a few costs (sanity)\n",
    "    print(\"\\nSample episode costs:\")\n",
    "    print(\"DQN:     \", [round(x, 1) for x in dqn_costs[:10]])\n",
    "    print(\"Baseline:\", [round(x, 1) for x in base_costs[:10]])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
