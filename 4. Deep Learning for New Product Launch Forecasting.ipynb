{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674834f5-1e36-4702-8ea6-8b8aeaf3fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# File: lesson_04_new_product_launch_forecasting_dl.py\n",
    "# Topic: Deep Learning for New Product Launch Forecasting (cold-start demand)\n",
    "# Input: historical SKUs (attributes + weekly demand) + new SKU attributes only\n",
    "# Output: (1) cold-start forecast, (2) top-K similar SKUs via Siamese Networks\n",
    "# =============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9ffe6c-a609-40d7-b4b8-cac0fd22014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 0. TOP-LEVEL CONFIG AND CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "SEED = 42\n",
    "LAUNCH_HORIZON_WEEKS = 12\n",
    "\n",
    "# Synthetic dataset sizes\n",
    "N_EXISTING_SKUS = 600\n",
    "N_NEW_SKUS = 8\n",
    "\n",
    "# Attribute schema (categorical + numeric)\n",
    "CATEGORIES = [\"Beverage\", \"Snack\", \"Dairy\", \"HomeCare\", \"PersonalCare\"]\n",
    "PACK_SIZES = [\"S\", \"M\", \"L\"]\n",
    "BRANDS = [\"Value\", \"Core\", \"Premium\"]\n",
    "\n",
    "# Numeric features: must match encode_attributes() exactly\n",
    "# [price_index, promo_intensity, innovation_score, distribution_points_scaled]\n",
    "NUMERIC_DIM = 4\n",
    "\n",
    "# Business demo settings\n",
    "SERVICE_LEVEL = 0.95\n",
    "TOP_K_SIMILARS = 5\n",
    "\n",
    "# Training settings\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_BASE = 15\n",
    "EPOCHS_SIAMESE = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Embedding sizes (categoricals)\n",
    "EMB_DIM_CATEGORY = 4\n",
    "EMB_DIM_PACK = 3\n",
    "EMB_DIM_BRAND = 3\n",
    "\n",
    "# Regularization\n",
    "DROPOUT_RATE = 0.15\n",
    "L2 = 1e-5\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ProductAttributes:\n",
    "    sku_id: str\n",
    "    category: str\n",
    "    pack_size: str\n",
    "    brand_tier: str\n",
    "    price_index: float\n",
    "    promo_intensity: float\n",
    "    innovation_score: float\n",
    "    distribution_points: int\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SkuWeeklyDemand:\n",
    "    sku_id: str\n",
    "    demand_weekly: np.ndarray  # shape: [weeks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9fb2116-cd40-48d8-95cb-1d5a6d8b4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ProductAttributes:\n",
    "    sku_id: str\n",
    "    category: str\n",
    "    pack_size: str\n",
    "    brand_tier: str\n",
    "    price_index: float\n",
    "    promo_intensity: float\n",
    "    innovation_score: float\n",
    "    distribution_points: int\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SkuWeeklyDemand:\n",
    "    sku_id: str\n",
    "    demand_weekly: np.ndarray  # shape: [weeks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b9bbb8-dbda-40e6-a657-773f26bba887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def set_global_determinism(seed: int) -> None:\n",
    "    \"\"\"Fix random seeds for reproducibility.\"\"\"\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def estimate_launch_fill_rate(forecast_weekly: np.ndarray, initial_inventory: float) -> float:\n",
    "    \"\"\"Simple fill-rate proxy across the launch horizon.\"\"\"\n",
    "    total_demand = float(np.sum(forecast_weekly))\n",
    "    if total_demand <= 1e-6:\n",
    "        return 1.0\n",
    "    return float(min(1.0, initial_inventory / total_demand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f98020-cfab-450e-ac8f-ef51f0bab30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. SYNTHETIC DATA GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def sample_attributes(sku_id: str) -> ProductAttributes:\n",
    "    \"\"\"Sample plausible attributes for a SKU.\"\"\"\n",
    "    category = random.choice(CATEGORIES)\n",
    "    pack_size = random.choice(PACK_SIZES)\n",
    "    brand_tier = random.choice(BRANDS)\n",
    "\n",
    "    price_index = float(np.clip(np.random.normal(loc=1.0, scale=0.15), 0.6, 1.6))\n",
    "    promo_intensity = float(np.clip(np.random.beta(a=2.0, b=4.5), 0.0, 1.0))\n",
    "    innovation_score = float(np.clip(np.random.beta(a=2.5, b=2.5), 0.0, 1.0))\n",
    "    distribution_points = int(np.clip(np.random.normal(loc=120, scale=50), 20, 260))\n",
    "\n",
    "    return ProductAttributes(\n",
    "        sku_id=sku_id,\n",
    "        category=category,\n",
    "        pack_size=pack_size,\n",
    "        brand_tier=brand_tier,\n",
    "        price_index=price_index,\n",
    "        promo_intensity=promo_intensity,\n",
    "        innovation_score=innovation_score,\n",
    "        distribution_points=distribution_points,\n",
    "    )\n",
    "\n",
    "\n",
    "def category_base_demand(category: str) -> float:\n",
    "    \"\"\"Different categories have different baseline volumes.\"\"\"\n",
    "    mapping = {\n",
    "        \"Beverage\": 900.0,\n",
    "        \"Snack\": 750.0,\n",
    "        \"Dairy\": 620.0,\n",
    "        \"HomeCare\": 420.0,\n",
    "        \"PersonalCare\": 380.0,\n",
    "    }\n",
    "    return float(mapping[category])\n",
    "\n",
    "\n",
    "def simulate_adoption_curve(weeks: int, innovation_score: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Logistic adoption curve (ramp-up then plateau).\n",
    "    \"\"\"\n",
    "    t = np.arange(weeks)\n",
    "    k = 0.25 + 0.9 * innovation_score\n",
    "    midpoint = 3 + (1.0 - innovation_score) * 4\n",
    "    curve = 1.0 / (1.0 + np.exp(-k * (t - midpoint)))\n",
    "    return curve.astype(np.float32)\n",
    "\n",
    "\n",
    "def generate_weekly_demand(attrs: ProductAttributes, weeks: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate weekly demand based on attributes.\n",
    "    \"\"\"\n",
    "    base = category_base_demand(attrs.category)\n",
    "\n",
    "    # Distribution effect: more points => more reach\n",
    "    dist_mult = np.interp(attrs.distribution_points, [20, 260], [0.4, 1.3])\n",
    "\n",
    "    # Price effect: higher price reduces volume\n",
    "    price_mult = (1.0 / attrs.price_index) ** 0.6\n",
    "\n",
    "    adoption = simulate_adoption_curve(weeks, attrs.innovation_score)\n",
    "\n",
    "    # Promo weeks and uplift\n",
    "    is_promo = np.random.binomial(n=1, p=attrs.promo_intensity, size=weeks)\n",
    "    promo_uplift = 1.0 + is_promo * np.random.uniform(0.10, 0.40, size=weeks)\n",
    "\n",
    "    noise = np.random.normal(loc=1.0, scale=0.08, size=weeks)\n",
    "\n",
    "    demand = base * dist_mult * price_mult * adoption * promo_uplift * noise\n",
    "    demand = np.clip(demand, 0.0, None)\n",
    "\n",
    "    return demand.astype(np.float32)\n",
    "\n",
    "\n",
    "def build_synthetic_dataset(\n",
    "    n_existing_skus: int,\n",
    "    n_new_skus: int,\n",
    "    weeks: int,\n",
    ") -> Tuple[List[ProductAttributes], List[SkuWeeklyDemand], List[ProductAttributes]]:\n",
    "    \"\"\"Build existing SKUs with history + new SKUs with only attributes.\"\"\"\n",
    "    existing_attrs: List[ProductAttributes] = []\n",
    "    existing_demand: List[SkuWeeklyDemand] = []\n",
    "    new_attrs: List[ProductAttributes] = []\n",
    "\n",
    "    for i in range(n_existing_skus):\n",
    "        sku_id = f\"SKU_{i:04d}\"\n",
    "        attrs = sample_attributes(sku_id)\n",
    "        demand = generate_weekly_demand(attrs, weeks)\n",
    "        existing_attrs.append(attrs)\n",
    "        existing_demand.append(SkuWeeklyDemand(sku_id=sku_id, demand_weekly=demand))\n",
    "\n",
    "    for j in range(n_new_skus):\n",
    "        sku_id = f\"NEW_{j:03d}\"\n",
    "        new_attrs.append(sample_attributes(sku_id))\n",
    "\n",
    "    return existing_attrs, existing_demand, new_attrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb80d691-626b-4870-b6e1-a46fe2b5f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. ENCODING (attributes -> tensors)\n",
    "# =============================================================================\n",
    "\n",
    "def build_vocab(values: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"String-to-index mapping for categorical attributes.\"\"\"\n",
    "    return {v: i for i, v in enumerate(sorted(set(values)))}\n",
    "\n",
    "\n",
    "def encode_attributes(\n",
    "    attrs_list: List[ProductAttributes],\n",
    "    vocab_category: Dict[str, int],\n",
    "    vocab_pack: Dict[str, int],\n",
    "    vocab_brand: Dict[str, int],\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Produce model-ready inputs.\n",
    "\n",
    "    Why:\n",
    "    - Categorical -> embeddings\n",
    "    - Numeric -> scaled floats\n",
    "    \"\"\"\n",
    "    category_idx = np.array([vocab_category[a.category] for a in attrs_list], dtype=np.int32)\n",
    "    pack_idx = np.array([vocab_pack[a.pack_size] for a in attrs_list], dtype=np.int32)\n",
    "    brand_idx = np.array([vocab_brand[a.brand_tier] for a in attrs_list], dtype=np.int32)\n",
    "\n",
    "    price_index = np.array([a.price_index for a in attrs_list], dtype=np.float32)\n",
    "    promo_intensity = np.array([a.promo_intensity for a in attrs_list], dtype=np.float32)\n",
    "    innovation_score = np.array([a.innovation_score for a in attrs_list], dtype=np.float32)\n",
    "\n",
    "    # Scale distribution points to 0..1\n",
    "    distribution_points = np.array([a.distribution_points for a in attrs_list], dtype=np.float32)\n",
    "    distribution_scaled = (distribution_points - 20.0) / (260.0 - 20.0)\n",
    "\n",
    "    numeric_features = np.stack(\n",
    "        [price_index, promo_intensity, innovation_score, distribution_scaled],\n",
    "        axis=1\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    return {\n",
    "        \"category_idx\": category_idx,\n",
    "        \"pack_idx\": pack_idx,\n",
    "        \"brand_idx\": brand_idx,\n",
    "        \"numeric_features\": numeric_features,\n",
    "    }\n",
    "\n",
    "\n",
    "def demand_targets(existing_demand: List[SkuWeeklyDemand]) -> np.ndarray:\n",
    "    \"\"\"Stack demand history into shape [n_skus, weeks].\"\"\"\n",
    "    return np.stack([d.demand_weekly for d in existing_demand], axis=0).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42227313-1b09-4ba1-806e-21563ae2eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. MODEL BUILDING (encoder, forecaster, siamese)\n",
    "# =============================================================================\n",
    "\n",
    "def build_product_encoder(\n",
    "    n_categories: int,\n",
    "    n_packs: int,\n",
    "    n_brands: int,\n",
    "    numeric_dim: int,\n",
    "    embedding_dim_out: int = 16,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Product encoder = product embedding model.\n",
    "\n",
    "    Inputs are explicitly named. This matters for dict-based training/inference.\n",
    "    \"\"\"\n",
    "    category_in = tf.keras.Input(shape=(), dtype=tf.int32, name=\"category_idx\")\n",
    "    pack_in = tf.keras.Input(shape=(), dtype=tf.int32, name=\"pack_idx\")\n",
    "    brand_in = tf.keras.Input(shape=(), dtype=tf.int32, name=\"brand_idx\")\n",
    "    numeric_in = tf.keras.Input(shape=(numeric_dim,), dtype=tf.float32, name=\"numeric_features\")\n",
    "\n",
    "    category_emb = tf.keras.layers.Embedding(n_categories, EMB_DIM_CATEGORY, name=\"emb_category\")(category_in)\n",
    "    pack_emb = tf.keras.layers.Embedding(n_packs, EMB_DIM_PACK, name=\"emb_pack\")(pack_in)\n",
    "    brand_emb = tf.keras.layers.Embedding(n_brands, EMB_DIM_BRAND, name=\"emb_brand\")(brand_in)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(name=\"concat_all\")([category_emb, pack_emb, brand_emb, numeric_in])\n",
    "\n",
    "    x = tf.keras.layers.Dense(\n",
    "        32, activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(L2),\n",
    "        name=\"enc_dense_1\"\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT_RATE, name=\"enc_dropout\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(\n",
    "        embedding_dim_out, activation=None,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(L2),\n",
    "        name=\"enc_dense_out\"\n",
    "    )(x)\n",
    "\n",
    "    # Normalize embedding so cosine similarity works reliably\n",
    "    out = tf.keras.layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1), name=\"l2_norm\")(x)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs={\n",
    "            \"category_idx\": category_in,\n",
    "            \"pack_idx\": pack_in,\n",
    "            \"brand_idx\": brand_in,\n",
    "            \"numeric_features\": numeric_in,\n",
    "        },\n",
    "        outputs=out,\n",
    "        name=\"product_encoder\",\n",
    "    )\n",
    "\n",
    "\n",
    "def build_attribute_forecaster(encoder: tf.keras.Model, horizon_weeks: int) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Attribute-based demand forecaster.\n",
    "\n",
    "    Important:\n",
    "    - We train on log1p(demand) to avoid flat/too-small predictions.\n",
    "    - Output is linear and represents log1p(demand).\n",
    "    \"\"\"\n",
    "    emb = encoder.output\n",
    "\n",
    "    x = tf.keras.layers.Dense(\n",
    "        32, activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(L2),\n",
    "        name=\"fc_dense_1\"\n",
    "    )(emb)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT_RATE, name=\"fc_dropout\")(x)\n",
    "\n",
    "    y_log = tf.keras.layers.Dense(\n",
    "        horizon_weeks, activation=\"linear\",\n",
    "        name=\"forecast_log_demand\"\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=encoder.inputs, outputs=y_log, name=\"attribute_forecaster\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=\"mae\",\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_siamese_network(encoder: tf.keras.Model, numeric_dim: int) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Siamese network (pairwise similarity classification).\n",
    "\n",
    "    Key implementation choice:\n",
    "    - Siamese model inputs are named left_* and right_*.\n",
    "    - That must match the keys you provide to siamese.fit(...).\n",
    "    \"\"\"\n",
    "    # Left inputs\n",
    "    left_category = tf.keras.Input(shape=(), dtype=tf.int32, name=\"left_category_idx\")\n",
    "    left_pack = tf.keras.Input(shape=(), dtype=tf.int32, name=\"left_pack_idx\")\n",
    "    left_brand = tf.keras.Input(shape=(), dtype=tf.int32, name=\"left_brand_idx\")\n",
    "    left_numeric = tf.keras.Input(shape=(numeric_dim,), dtype=tf.float32, name=\"left_numeric_features\")\n",
    "\n",
    "    # Right inputs\n",
    "    right_category = tf.keras.Input(shape=(), dtype=tf.int32, name=\"right_category_idx\")\n",
    "    right_pack = tf.keras.Input(shape=(), dtype=tf.int32, name=\"right_pack_idx\")\n",
    "    right_brand = tf.keras.Input(shape=(), dtype=tf.int32, name=\"right_brand_idx\")\n",
    "    right_numeric = tf.keras.Input(shape=(numeric_dim,), dtype=tf.float32, name=\"right_numeric_features\")\n",
    "\n",
    "    left_for_encoder = {\n",
    "        \"category_idx\": left_category,\n",
    "        \"pack_idx\": left_pack,\n",
    "        \"brand_idx\": left_brand,\n",
    "        \"numeric_features\": left_numeric,\n",
    "    }\n",
    "    right_for_encoder = {\n",
    "        \"category_idx\": right_category,\n",
    "        \"pack_idx\": right_pack,\n",
    "        \"brand_idx\": right_brand,\n",
    "        \"numeric_features\": right_numeric,\n",
    "    }\n",
    "\n",
    "    # Shared tower embeddings\n",
    "    left_emb = encoder(left_for_encoder)\n",
    "    right_emb = encoder(right_for_encoder)\n",
    "\n",
    "    # Cosine distance: 1 - cosine similarity\n",
    "    dist = tf.keras.layers.Lambda(\n",
    "        lambda z: 1.0 - tf.reduce_sum(z[0] * z[1], axis=1, keepdims=True),\n",
    "        name=\"cosine_dist\"\n",
    "    )([left_emb, right_emb])\n",
    "\n",
    "    prob_similar = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"prob_similar\")(dist)\n",
    "\n",
    "    siamese = tf.keras.Model(\n",
    "        inputs={\n",
    "            \"left_category_idx\": left_category,\n",
    "            \"left_pack_idx\": left_pack,\n",
    "            \"left_brand_idx\": left_brand,\n",
    "            \"left_numeric_features\": left_numeric,\n",
    "            \"right_category_idx\": right_category,\n",
    "            \"right_pack_idx\": right_pack,\n",
    "            \"right_brand_idx\": right_brand,\n",
    "            \"right_numeric_features\": right_numeric,\n",
    "        },\n",
    "        outputs=prob_similar,\n",
    "        name=\"siamese_similarity_model\",\n",
    "    )\n",
    "\n",
    "    siamese.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC(name=\"auc\")],\n",
    "    )\n",
    "    return siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e9392c-f272-40f7-a4d8-23764214be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. SIAMESE PAIR LABELS + SAMPLING\n",
    "# =============================================================================\n",
    "\n",
    "def make_similarity_labels_from_demand(demand: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create pseudo similarity labels from demand curve shape.\n",
    "\n",
    "    Approach:\n",
    "    - Normalize each SKU curve by its mean.\n",
    "    - Define “similar” as being in the closest 15% pairs by L2 distance.\n",
    "    \"\"\"\n",
    "    demand_norm = demand / (np.mean(demand, axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "    n = demand_norm.shape[0]\n",
    "    labels = np.zeros((n, n), dtype=np.int32)\n",
    "\n",
    "    # Compute distance distribution (toy O(n^2))\n",
    "    dists = []\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dists.append(np.linalg.norm(demand_norm[i] - demand_norm[j]))\n",
    "\n",
    "    threshold = float(np.quantile(np.array(dists), 0.15))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            d = float(np.linalg.norm(demand_norm[i] - demand_norm[j]))\n",
    "            lab = 1 if d <= threshold else 0\n",
    "            labels[i, j] = lab\n",
    "            labels[j, i] = lab\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def sample_pairs(\n",
    "    encoded_existing: Dict[str, np.ndarray],\n",
    "    labels: np.ndarray,\n",
    "    n_pairs: int,\n",
    "    positive_ratio: float,\n",
    ") -> Tuple[Dict[str, np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Sample pairs for Siamese training.\n",
    "\n",
    "    Output keys match Siamese model inputs exactly.\n",
    "    \"\"\"\n",
    "    n = labels.shape[0]\n",
    "\n",
    "    pos_pairs: List[Tuple[int, int]] = []\n",
    "    neg_pairs: List[Tuple[int, int]] = []\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if labels[i, j] == 1:\n",
    "                pos_pairs.append((i, j))\n",
    "            else:\n",
    "                neg_pairs.append((i, j))\n",
    "\n",
    "    n_pos = int(n_pairs * positive_ratio)\n",
    "    n_neg = n_pairs - n_pos\n",
    "\n",
    "    pos_sample = random.sample(pos_pairs, k=min(n_pos, len(pos_pairs)))\n",
    "    neg_sample = random.sample(neg_pairs, k=min(n_neg, len(neg_pairs)))\n",
    "\n",
    "    pairs = pos_sample + neg_sample\n",
    "    random.shuffle(pairs)\n",
    "\n",
    "    y = np.array([1.0 if labels[i, j] == 1 else 0.0 for i, j in pairs], dtype=np.float32)\n",
    "\n",
    "    pair_X = {\n",
    "        \"left_category_idx\": np.array([encoded_existing[\"category_idx\"][i] for i, _ in pairs], dtype=np.int32),\n",
    "        \"right_category_idx\": np.array([encoded_existing[\"category_idx\"][j] for _, j in pairs], dtype=np.int32),\n",
    "        \"left_pack_idx\": np.array([encoded_existing[\"pack_idx\"][i] for i, _ in pairs], dtype=np.int32),\n",
    "        \"right_pack_idx\": np.array([encoded_existing[\"pack_idx\"][j] for _, j in pairs], dtype=np.int32),\n",
    "        \"left_brand_idx\": np.array([encoded_existing[\"brand_idx\"][i] for i, _ in pairs], dtype=np.int32),\n",
    "        \"right_brand_idx\": np.array([encoded_existing[\"brand_idx\"][j] for _, j in pairs], dtype=np.int32),\n",
    "        \"left_numeric_features\": np.array([encoded_existing[\"numeric_features\"][i] for i, _ in pairs], dtype=np.float32),\n",
    "        \"right_numeric_features\": np.array([encoded_existing[\"numeric_features\"][j] for _, j in pairs], dtype=np.float32),\n",
    "    }\n",
    "\n",
    "    return pair_X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e9ba3b-a9f1-4176-b6f9-8e537cd7e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. INFERENCE HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "def predict_weekly_demand_from_log(forecaster: tf.keras.Model, X: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Convert log1p predictions back into demand.\"\"\"\n",
    "    y_log = forecaster.predict(X, verbose=0).astype(np.float32)\n",
    "    y = np.expm1(y_log)\n",
    "    return np.clip(y, 0.0, None).astype(np.float32)\n",
    "\n",
    "\n",
    "def top_k_similar_skus(\n",
    "    encoder: tf.keras.Model,\n",
    "    X_existing: Dict[str, np.ndarray],\n",
    "    X_new: Dict[str, np.ndarray],\n",
    "    existing_ids: List[str],\n",
    "    new_ids: List[str],\n",
    "    top_k: int,\n",
    ") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"Top-K retrieval by embedding cosine similarity.\"\"\"\n",
    "    emb_existing = encoder.predict(X_existing, verbose=0)\n",
    "    emb_new = encoder.predict(X_new, verbose=0)\n",
    "\n",
    "    sim = np.matmul(emb_new, emb_existing.T)\n",
    "\n",
    "    result: Dict[str, List[Tuple[str, float]]] = {}\n",
    "    for i, new_id in enumerate(new_ids):\n",
    "        idx = np.argsort(-sim[i])[:top_k]\n",
    "        result[new_id] = [(existing_ids[j], float(sim[i, j])) for j in idx]\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7058e89a-6dac-45ba-9ee8-3c1d335d9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. PROGRESSIVE EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "def example_1_attribute_forecasting_only() -> None:\n",
    "    existing_attrs, existing_demand, new_attrs = build_synthetic_dataset(\n",
    "        n_existing_skus=N_EXISTING_SKUS,\n",
    "        n_new_skus=N_NEW_SKUS,\n",
    "        weeks=LAUNCH_HORIZON_WEEKS,\n",
    "    )\n",
    "\n",
    "    vocab_category = build_vocab([a.category for a in existing_attrs + new_attrs])\n",
    "    vocab_pack = build_vocab([a.pack_size for a in existing_attrs + new_attrs])\n",
    "    vocab_brand = build_vocab([a.brand_tier for a in existing_attrs + new_attrs])\n",
    "\n",
    "    X_existing = encode_attributes(existing_attrs, vocab_category, vocab_pack, vocab_brand)\n",
    "    y_existing = demand_targets(existing_demand)\n",
    "    y_existing_log = np.log1p(y_existing).astype(np.float32)\n",
    "\n",
    "    X_new = encode_attributes(new_attrs, vocab_category, vocab_pack, vocab_brand)\n",
    "\n",
    "    encoder = build_product_encoder(\n",
    "        n_categories=len(vocab_category),\n",
    "        n_packs=len(vocab_pack),\n",
    "        n_brands=len(vocab_brand),\n",
    "        numeric_dim=NUMERIC_DIM,\n",
    "        embedding_dim_out=16,\n",
    "    )\n",
    "    forecaster = build_attribute_forecaster(encoder, horizon_weeks=LAUNCH_HORIZON_WEEKS)\n",
    "\n",
    "    forecaster.fit(\n",
    "        X_existing,\n",
    "        y_existing_log,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS_BASE,\n",
    "        validation_split=0.15,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    preds_new = predict_weekly_demand_from_log(forecaster, X_new)\n",
    "\n",
    "    print(\"\\n=== Example 1: Attribute-based forecasting (cold-start) ===\")\n",
    "    for i, attrs in enumerate(new_attrs[:3]):\n",
    "        weekly = preds_new[i]\n",
    "        total = float(np.sum(weekly))\n",
    "        avg = float(np.mean(weekly))\n",
    "\n",
    "        print(f\"\\nNew SKU: {attrs.sku_id} | {attrs.category}/{attrs.brand_tier}/{attrs.pack_size}\")\n",
    "        print(f\"  Forecast total (12w): {total:,.0f} units | Avg weekly: {avg:,.0f}\")\n",
    "\n",
    "        initial_inventory = 0.7 * total\n",
    "        fill_rate = estimate_launch_fill_rate(weekly, initial_inventory=initial_inventory)\n",
    "        print(f\"  Launch fill-rate proxy (inventory=70% of forecast): {fill_rate:.2%}\")\n",
    "\n",
    "\n",
    "def example_2_transfer_learning_freeze_encoder() -> None:\n",
    "    existing_attrs, existing_demand, new_attrs = build_synthetic_dataset(\n",
    "        n_existing_skus=500,\n",
    "        n_new_skus=3,\n",
    "        weeks=LAUNCH_HORIZON_WEEKS,\n",
    "    )\n",
    "\n",
    "    vocab_category = build_vocab([a.category for a in existing_attrs + new_attrs])\n",
    "    vocab_pack = build_vocab([a.pack_size for a in existing_attrs + new_attrs])\n",
    "    vocab_brand = build_vocab([a.brand_tier for a in existing_attrs + new_attrs])\n",
    "\n",
    "    X_existing = encode_attributes(existing_attrs, vocab_category, vocab_pack, vocab_brand)\n",
    "    y_existing = demand_targets(existing_demand)\n",
    "    y_existing_log = np.log1p(y_existing).astype(np.float32)\n",
    "\n",
    "    X_new = encode_attributes(new_attrs, vocab_category, vocab_pack, vocab_brand)\n",
    "\n",
    "    encoder = build_product_encoder(\n",
    "        n_categories=len(vocab_category),\n",
    "        n_packs=len(vocab_pack),\n",
    "        n_brands=len(vocab_brand),\n",
    "        numeric_dim=NUMERIC_DIM,\n",
    "        embedding_dim_out=16,\n",
    "    )\n",
    "    forecaster = build_attribute_forecaster(encoder, horizon_weeks=LAUNCH_HORIZON_WEEKS)\n",
    "\n",
    "    forecaster.fit(X_existing, y_existing_log, batch_size=BATCH_SIZE, epochs=10, verbose=0)\n",
    "\n",
    "    # Freeze encoder for small-data fine-tuning\n",
    "    encoder.trainable = False\n",
    "    forecaster.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
    "        loss=\"mae\",\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    "    )\n",
    "\n",
    "    idx = np.random.choice(len(existing_attrs), size=120, replace=False)\n",
    "    X_small = {k: v[idx] for k, v in X_existing.items()}\n",
    "    y_small_log = y_existing_log[idx]\n",
    "\n",
    "    forecaster.fit(X_small, y_small_log, batch_size=BATCH_SIZE, epochs=6, verbose=0)\n",
    "\n",
    "    preds_new = predict_weekly_demand_from_log(forecaster, X_new)\n",
    "\n",
    "    print(\"\\n=== Example 2: Transfer learning (freeze encoder, fine-tune head) ===\")\n",
    "    for i, attrs in enumerate(new_attrs):\n",
    "        print(\n",
    "            f\"{attrs.sku_id}: total={np.sum(preds_new[i]):,.0f} | \"\n",
    "            f\"cat={attrs.category}, brand={attrs.brand_tier}, pack={attrs.pack_size}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def example_3_siamese_similarity_and_analog_forecast() -> None:\n",
    "    existing_attrs, existing_demand, new_attrs = build_synthetic_dataset(\n",
    "        n_existing_skus=450,\n",
    "        n_new_skus=5,\n",
    "        weeks=LAUNCH_HORIZON_WEEKS,\n",
    "    )\n",
    "\n",
    "    vocab_category = build_vocab([a.category for a in existing_attrs + new_attrs])\n",
    "    vocab_pack = build_vocab([a.pack_size for a in existing_attrs + new_attrs])\n",
    "    vocab_brand = build_vocab([a.brand_tier for a in existing_attrs + new_attrs])\n",
    "\n",
    "    X_existing = encode_attributes(existing_attrs, vocab_category, vocab_pack, vocab_brand)\n",
    "    y_existing = demand_targets(existing_demand)\n",
    "    y_existing_log = np.log1p(y_existing).astype(np.float32)\n",
    "\n",
    "    X_new = encode_attributes(new_attrs, vocab_category, vocab_pack, vocab_brand)\n",
    "\n",
    "    existing_ids = [a.sku_id for a in existing_attrs]\n",
    "    new_ids = [a.sku_id for a in new_attrs]\n",
    "\n",
    "    # Pre-train encoder using forecasting task\n",
    "    encoder = build_product_encoder(\n",
    "        n_categories=len(vocab_category),\n",
    "        n_packs=len(vocab_pack),\n",
    "        n_brands=len(vocab_brand),\n",
    "        numeric_dim=NUMERIC_DIM,\n",
    "        embedding_dim_out=16,\n",
    "    )\n",
    "    forecaster = build_attribute_forecaster(encoder, horizon_weeks=LAUNCH_HORIZON_WEEKS)\n",
    "    forecaster.fit(X_existing, y_existing_log, batch_size=BATCH_SIZE, epochs=10, verbose=0)\n",
    "\n",
    "    # Siamese training on pseudo-labels\n",
    "    labels = make_similarity_labels_from_demand(y_existing)\n",
    "    pair_X, pair_y = sample_pairs(X_existing, labels, n_pairs=7000, positive_ratio=0.5)\n",
    "\n",
    "    siamese = build_siamese_network(encoder, numeric_dim=NUMERIC_DIM)\n",
    "    siamese.fit(pair_X, pair_y, batch_size=BATCH_SIZE, epochs=EPOCHS_SIAMESE, verbose=0)\n",
    "\n",
    "    # Similar SKUs by embedding cosine similarity\n",
    "    topk = top_k_similar_skus(\n",
    "        encoder=encoder,\n",
    "        X_existing=X_existing,\n",
    "        X_new=X_new,\n",
    "        existing_ids=existing_ids,\n",
    "        new_ids=new_ids,\n",
    "        top_k=TOP_K_SIMILARS,\n",
    "    )\n",
    "\n",
    "    preds_new = predict_weekly_demand_from_log(forecaster, X_new)\n",
    "\n",
    "    print(\"\\n=== Example 3: Siamese similarity + analog explanation ===\")\n",
    "    for i, attrs in enumerate(new_attrs[:3]):\n",
    "        analog_ids = [sid for sid, _ in topk[attrs.sku_id]]\n",
    "        analog_idx = [existing_ids.index(sid) for sid in analog_ids]\n",
    "\n",
    "        analog_curve = np.mean(y_existing[analog_idx], axis=0)\n",
    "        model_curve = preds_new[i]\n",
    "\n",
    "        print(f\"\\nNew SKU: {attrs.sku_id} | {attrs.category}/{attrs.brand_tier}/{attrs.pack_size}\")\n",
    "        print(\"  Top similar SKUs (id, cosine sim):\")\n",
    "        for sid, s in topk[attrs.sku_id]:\n",
    "            print(f\"   - {sid} | {s:.3f}\")\n",
    "\n",
    "        print(f\"  Model forecast total:  {np.sum(model_curve):,.0f}\")\n",
    "        print(f\"  Analog forecast total: {np.sum(analog_curve):,.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7c5bb38-15f7-4218-8085-db7bc7cde5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. WHEN TO USE WHAT (recommendations)\n",
    "# =============================================================================\n",
    "\n",
    "def recommendations_when_to_use_which() -> None:\n",
    "    print(\"\\n=== Recommendations: when to use what ===\")\n",
    "    print(\"1) Attribute NN: best default for scalable cold-start forecasting.\")\n",
    "    print(\"2) Transfer learning: best when new data is small or market shifts.\")\n",
    "    print(\"3) Siamese similarity: best for analog explanation and portfolio logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ea3198-4552-4a43-9be4-d23b714c70a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      "=== Example 1: Attribute-based forecasting (cold-start) ===\n",
      "\n",
      "New SKU: NEW_000 | Snack/Core/M\n",
      "  Forecast total (12w): 459 units | Avg weekly: 38\n",
      "  Launch fill-rate proxy (inventory=70% of forecast): 70.00%\n",
      "\n",
      "New SKU: NEW_001 | HomeCare/Premium/S\n",
      "  Forecast total (12w): 463 units | Avg weekly: 39\n",
      "  Launch fill-rate proxy (inventory=70% of forecast): 70.00%\n",
      "\n",
      "New SKU: NEW_002 | Beverage/Premium/M\n",
      "  Forecast total (12w): 463 units | Avg weekly: 39\n",
      "  Launch fill-rate proxy (inventory=70% of forecast): 70.00%\n",
      "\n",
      "=== Example 2: Transfer learning (freeze encoder, fine-tune head) ===\n",
      "NEW_000: total=40 | cat=PersonalCare, brand=Core, pack=M\n",
      "NEW_001: total=41 | cat=Snack, brand=Premium, pack=M\n",
      "NEW_002: total=41 | cat=Snack, brand=Premium, pack=S\n",
      "\n",
      "=== Example 3: Siamese similarity + analog explanation ===\n",
      "\n",
      "New SKU: NEW_000 | Dairy/Premium/M\n",
      "  Top similar SKUs (id, cosine sim):\n",
      "   - SKU_0067 | 0.999\n",
      "   - SKU_0230 | 0.997\n",
      "   - SKU_0441 | 0.995\n",
      "   - SKU_0012 | 0.995\n",
      "   - SKU_0386 | 0.994\n",
      "  Model forecast total:  17\n",
      "  Analog forecast total: 3,303\n",
      "\n",
      "New SKU: NEW_001 | PersonalCare/Premium/L\n",
      "  Top similar SKUs (id, cosine sim):\n",
      "   - SKU_0369 | 0.994\n",
      "   - SKU_0073 | 0.989\n",
      "   - SKU_0366 | 0.987\n",
      "   - SKU_0341 | 0.987\n",
      "   - SKU_0329 | 0.986\n",
      "  Model forecast total:  26\n",
      "  Analog forecast total: 3,139\n",
      "\n",
      "New SKU: NEW_002 | Dairy/Premium/M\n",
      "  Top similar SKUs (id, cosine sim):\n",
      "   - SKU_0307 | 0.997\n",
      "   - SKU_0255 | 0.996\n",
      "   - SKU_0389 | 0.996\n",
      "   - SKU_0329 | 0.995\n",
      "   - SKU_0402 | 0.993\n",
      "  Model forecast total:  22\n",
      "  Analog forecast total: 3,477\n",
      "\n",
      "=== Recommendations: when to use what ===\n",
      "1) Attribute NN: best default for scalable cold-start forecasting.\n",
      "2) Transfer learning: best when new data is small or market shifts.\n",
      "3) Siamese similarity: best for analog explanation and portfolio logic.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 10. DEMO / BUILT-IN CHECKS (run examples end-to-end)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_global_determinism(SEED)\n",
    "\n",
    "    example_1_attribute_forecasting_only()\n",
    "    example_2_transfer_learning_freeze_encoder()\n",
    "    example_3_siamese_similarity_and_analog_forecast()\n",
    "    recommendations_when_to_use_which()\n",
    "\n",
    "    # Try-yourself tasks:\n",
    "    # - TODO: Change TOP_K_SIMILARS and check analog stability.\n",
    "    # - TODO: Replace pseudo similarity labels with rule-based labels (category + price band).\n",
    "    # - TODO: Add a new categorical attribute (e.g., \"channel\") and re-train models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
