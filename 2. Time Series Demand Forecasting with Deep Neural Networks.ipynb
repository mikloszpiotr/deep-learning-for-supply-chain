{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ee054f-37f3-40a8-bc89-2454fa88263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# File: demand_forecasting_dl_tf.py\n",
    "# Topic: Time Series Demand Forecasting with Deep Neural Networks (TensorFlow)\n",
    "# Purpose: Train DL models (MLP, LSTM, GRU) to forecast demand using covariates.\n",
    "# Input: tabular time series (date, sku_id, location_id, demand, price, promo, ...)\n",
    "# Output: forecasts + business metrics (MAPE, Bias) + service-level impact estimate\n",
    "# =============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1dc8bb2-7373-4389-b01e-c9799ef00887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ==== 0. TOP-LEVEL CONFIG, CONSTANTS, AND REPRODUCIBILITY =====================\n",
    "# =============================================================================\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---- Data configuration (adjust to your project structure) -------------------\n",
    "DATA_PATH = \"data/demand.csv\"  # if you load real data later\n",
    "DATE_COL = \"date\"\n",
    "TARGET_COL = \"demand\"\n",
    "\n",
    "# Entity keys (for aggregation levels)\n",
    "SKU_COL = \"sku_id\"\n",
    "LOC_COL = \"location_id\"\n",
    "\n",
    "# Exogenous / covariate features for multivariate forecasting\n",
    "COVARIATE_COLS = [\n",
    "    \"price\",\n",
    "    \"promo_flag\",\n",
    "    \"holiday_flag\",\n",
    "    \"temp_c\",\n",
    "]\n",
    "\n",
    "# ---- Windowing / forecasting configuration ----------------------------------\n",
    "LOOKBACK = 28          # historical days used as input\n",
    "HORIZON = 7            # days ahead to forecast (multi-step)\n",
    "STRIDE = 1             # move window by 1 day\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "# ---- Model configuration -----------------------------------------------------\n",
    "HIDDEN_UNITS = 64\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# ---- Business configuration --------------------------------------------------\n",
    "SERVICE_LEVEL = 0.95   # target service level for inventory/service impact\n",
    "LEAD_TIME_DAYS = 7     # used for simplified service level impact estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cedb1a2-aa80-4f70-929d-f85f74193af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ==== 1. UTILITIES: METRICS AND BUSINESS IMPACT ===============================\n",
    "# =============================================================================\n",
    "\n",
    "def mape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-6) -> float:\n",
    "    \"\"\"\n",
    "    Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "    Why: MAPE is widely understood in demand planning and easy to communicate,\n",
    "    but it can be unstable when demand approaches zero. We protect with eps.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / denom) * 100.0)\n",
    "\n",
    "\n",
    "def bias_pct(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-6) -> float:\n",
    "    \"\"\"\n",
    "    Forecast bias in percent.\n",
    "\n",
    "    Why: Bias shows systematic over/under-forecasting, which directly impacts\n",
    "    inventory (overstock vs. stockouts). Positive means over-forecasting.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = np.maximum(np.mean(np.abs(y_true)), eps)\n",
    "    return float(np.mean(y_pred - y_true) / denom * 100.0)\n",
    "\n",
    "\n",
    "def z_from_service_level(service_level: float) -> float:\n",
    "    \"\"\"\n",
    "    Approximate z-score for common service levels.\n",
    "\n",
    "    Why: In SCM, service level is often translated into a z value for safety stock.\n",
    "    For simplicity we use a small lookup; replace with scipy if desired.\n",
    "    \"\"\"\n",
    "    lookup = {\n",
    "        0.50: 0.00,\n",
    "        0.84: 1.00,\n",
    "        0.90: 1.28,\n",
    "        0.95: 1.645,\n",
    "        0.97: 1.88,\n",
    "        0.98: 2.05,\n",
    "        0.99: 2.33,\n",
    "    }\n",
    "    # Fallback: clamp to nearest known\n",
    "    keys = np.array(sorted(lookup.keys()))\n",
    "    nearest = float(keys[np.argmin(np.abs(keys - service_level))])\n",
    "    return lookup[nearest]\n",
    "\n",
    "\n",
    "def safety_stock(\n",
    "    demand_std_per_day: float,\n",
    "    lead_time_days: int,\n",
    "    service_level: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Simplified safety stock estimate:\n",
    "      SS = z * sigma_demand_per_day * sqrt(lead_time)\n",
    "\n",
    "    Why: This gives learners a direct bridge from forecast quality (variance)\n",
    "    to inventory/service KPIs. In real life, use demand during lead time and\n",
    "    include lead time variability as well.\n",
    "    \"\"\"\n",
    "    z = z_from_service_level(service_level)\n",
    "    return float(z * demand_std_per_day * math.sqrt(max(lead_time_days, 1)))\n",
    "\n",
    "\n",
    "def service_level_impact_proxy(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    service_level: float,\n",
    "    lead_time_days: int,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Proxy to connect forecast errors to service-level/inventory impact.\n",
    "\n",
    "    Approach (simplified):\n",
    "    - Use forecast residual std as a proxy for demand uncertainty.\n",
    "    - Compute implied safety stock for the target service level.\n",
    "\n",
    "    Why: This is not a full inventory simulator, but it demonstrates\n",
    "    how “better forecasts -> lower uncertainty -> lower safety stock”.\n",
    "    \"\"\"\n",
    "    residual = np.asarray(y_true) - np.asarray(y_pred)\n",
    "    sigma = float(np.std(residual))\n",
    "    ss = safety_stock(sigma, lead_time_days, service_level)\n",
    "    return {\n",
    "        \"residual_std\": sigma,\n",
    "        \"implied_safety_stock_units\": ss,\n",
    "        \"service_level_target\": float(service_level),\n",
    "        \"lead_time_days\": float(lead_time_days),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87da9c40-e221-4d90-a22d-1e9c19a6e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ==== 2. DATA: SYNTHETIC GENERATOR (FOR LEARNING / DEMO) ======================\n",
    "# =============================================================================\n",
    "\n",
    "def make_synthetic_sku_location_data(\n",
    "    n_days: int = 365,\n",
    "    sku_id: str = \"SKU_001\",\n",
    "    location_id: str = \"LOC_A\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a realistic-ish demand series with covariates.\n",
    "\n",
    "    Why: Learners can run end-to-end without needing a private dataset.\n",
    "    We include:\n",
    "    - weekly seasonality\n",
    "    - price elasticity (higher price -> lower demand)\n",
    "    - promotion lift\n",
    "    - holidays spikes\n",
    "    - weather influence\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(\"2024-01-01\", periods=n_days, freq=\"D\")\n",
    "\n",
    "    weekly = 10 * np.sin(2 * np.pi * np.arange(n_days) / 7.0)\n",
    "    trend = np.linspace(0, 5, n_days)\n",
    "\n",
    "    price = 10 + 0.5 * np.sin(2 * np.pi * np.arange(n_days) / 30.0) + np.random.normal(0, 0.2, n_days)\n",
    "    promo_flag = (np.random.rand(n_days) < 0.10).astype(int)\n",
    "    holiday_flag = (np.random.rand(n_days) < 0.03).astype(int)\n",
    "    temp_c = 15 + 10 * np.sin(2 * np.pi * np.arange(n_days) / 365.0) + np.random.normal(0, 1.0, n_days)\n",
    "\n",
    "    # Demand construction (keep non-negative)\n",
    "    base = 50 + weekly + trend\n",
    "    price_effect = -3.0 * (price - np.mean(price))  # elasticity\n",
    "    promo_lift = 15.0 * promo_flag\n",
    "    holiday_spike = 20.0 * holiday_flag\n",
    "    weather_effect = 0.3 * (temp_c - np.mean(temp_c))\n",
    "\n",
    "    noise = np.random.normal(0, 5, n_days)\n",
    "    demand = np.maximum(0, base + price_effect + promo_lift + holiday_spike + weather_effect + noise)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        DATE_COL: dates,\n",
    "        SKU_COL: sku_id,\n",
    "        LOC_COL: location_id,\n",
    "        TARGET_COL: demand.astype(float),\n",
    "        \"price\": price.astype(float),\n",
    "        \"promo_flag\": promo_flag.astype(int),\n",
    "        \"holiday_flag\": holiday_flag.astype(int),\n",
    "        \"temp_c\": temp_c.astype(float),\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_multi_entity_dataset() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a small multi-SKU, multi-location dataset.\n",
    "\n",
    "    Why: In SCM you rarely forecast a single series. This supports:\n",
    "    - multiple aggregation levels (SKU-LOC, SKU, LOC, TOTAL)\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for sku in [\"SKU_001\", \"SKU_002\", \"SKU_003\"]:\n",
    "        for loc in [\"LOC_A\", \"LOC_B\"]:\n",
    "            frames.append(make_synthetic_sku_location_data(n_days=365, sku_id=sku, location_id=loc))\n",
    "    return pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b46c39c-008d-4ae3-bdd3-98058a71606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ==== 3. AGGREGATION LEVELS (MULTI-LEVEL FORECASTING) ==========================\n",
    "# =============================================================================\n",
    "\n",
    "def aggregate_demand(\n",
    "    df: pd.DataFrame,\n",
    "    level: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate to different hierarchy levels.\n",
    "\n",
    "    level:\n",
    "      - \"sku_loc\": forecast each SKU-Location series\n",
    "      - \"sku\":     forecast SKU total across locations\n",
    "      - \"loc\":     forecast Location total across SKUs\n",
    "      - \"total\":   forecast overall demand\n",
    "\n",
    "    Why: Businesses forecast at multiple levels for S&OP, replenishment, and capacity.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "\n",
    "    group_cols = [DATE_COL]  # always group by time\n",
    "    if level == \"sku_loc\":\n",
    "        group_cols += [SKU_COL, LOC_COL]\n",
    "    elif level == \"sku\":\n",
    "        group_cols += [SKU_COL]\n",
    "    elif level == \"loc\":\n",
    "        group_cols += [LOC_COL]\n",
    "    elif level == \"total\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown level: {level}\")\n",
    "\n",
    "    # For covariates: we use mean as a simple aggregation choice.\n",
    "    agg_map = {TARGET_COL: \"sum\"}\n",
    "    for c in COVARIATE_COLS:\n",
    "        agg_map[c] = \"mean\"\n",
    "\n",
    "    out = df.groupby(group_cols, as_index=False).agg(agg_map).sort_values(group_cols)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caaa0b9b-dadc-4c38-bc8c-bfde0aa7e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ==== 4. FEATURE ENGINEERING: WINDOWING FOR DL MODELS ==========================\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class WindowedData:\n",
    "    X: np.ndarray  # shape: [samples, lookback, features] or flattened for MLP\n",
    "    y: np.ndarray  # shape: [samples, horizon]\n",
    "    feature_names: List[str]\n",
    "\n",
    "\n",
    "def standardize_train_only(\n",
    "    train_array: np.ndarray,\n",
    "    val_array: np.ndarray,\n",
    "    test_array: np.ndarray,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Standardize using TRAIN statistics only.\n",
    "\n",
    "    Why: Prevents leakage. In forecasting, leakage can easily occur if you compute\n",
    "    normalization on the full timeline.\n",
    "    \"\"\"\n",
    "    mean = train_array.mean(axis=0, keepdims=True)\n",
    "    std = train_array.std(axis=0, keepdims=True) + 1e-8\n",
    "    return (train_array - mean) / std, (val_array - mean) / std, (test_array - mean) / std, mean, std\n",
    "\n",
    "\n",
    "def make_supervised_windows(\n",
    "    df_series: pd.DataFrame,\n",
    "    lookback: int,\n",
    "    horizon: int,\n",
    "    target_col: str,\n",
    "    covariate_cols: List[str],\n",
    ") -> WindowedData:\n",
    "    \"\"\"\n",
    "    Convert a single time series dataframe into supervised learning windows.\n",
    "\n",
    "    Output:\n",
    "      X: [N, lookback, F] where F = 1 (demand lag) + len(covariates)\n",
    "      y: [N, horizon] (multi-step direct forecasting)\n",
    "\n",
    "    Why:\n",
    "    - DL models learn patterns across time from sequences.\n",
    "    - Multi-step direct output is a practical baseline for SCM horizons.\n",
    "    \"\"\"\n",
    "    df_series = df_series.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    # Input features include lagged demand itself + covariates aligned by date.\n",
    "    feature_names = [target_col] + covariate_cols\n",
    "    values = df_series[feature_names].to_numpy(dtype=float)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    total_len = len(df_series)\n",
    "\n",
    "    for start in range(0, total_len - lookback - horizon + 1, STRIDE):\n",
    "        end_x = start + lookback\n",
    "        end_y = end_x + horizon\n",
    "\n",
    "        X_list.append(values[start:end_x, :])             # [lookback, F]\n",
    "        y_list.append(values[end_x:end_y, 0])             # demand only, [horizon]\n",
    "\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y = np.stack(y_list, axis=0)\n",
    "    return WindowedData(X=X, y=y, feature_names=feature_names)\n",
    "\n",
    "\n",
    "def train_val_test_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    train_ratio: float,\n",
    "    val_ratio: float,\n",
    ") -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Time-based split (no shuffling).\n",
    "\n",
    "    Why: In time series you must preserve ordering; random split breaks the\n",
    "    forecasting constraint and inflates performance.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val, y_val = X[n_train:n_train + n_val], y[n_train:n_train + n_val]\n",
    "    X_test, y_test = X[n_train + n_val:], y[n_train + n_val:]\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c92034-2f14-49ed-8ac5-3b9f32f5a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ==== 5. MODELS: MLP VS. RNN VS. LSTM VS. GRU =================================\n",
    "# =============================================================================\n",
    "\n",
    "def build_mlp(\n",
    "    lookback: int,\n",
    "    n_features: int,\n",
    "    horizon: int,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    MLP baseline: flatten the lookback window into a single vector.\n",
    "\n",
    "    Why:\n",
    "    - Often competitive for short horizons and stable patterns.\n",
    "    - Simpler and faster than recurrent models.\n",
    "    Trade-off:\n",
    "    - Does not explicitly model temporal order beyond what flattening preserves.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(lookback, n_features))\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = tf.keras.layers.Dense(HIDDEN_UNITS, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT)(x)\n",
    "    x = tf.keras.layers.Dense(HIDDEN_UNITS, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(horizon)(x)\n",
    "    return tf.keras.Model(inputs, outputs, name=\"MLP_Forecaster\")\n",
    "\n",
    "\n",
    "def build_simple_rnn(\n",
    "    lookback: int,\n",
    "    n_features: int,\n",
    "    horizon: int,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Simple RNN: a minimal recurrent baseline.\n",
    "\n",
    "    Why:\n",
    "    - Teaches recurrence conceptually.\n",
    "    Trade-off:\n",
    "    - Can struggle with longer dependencies due to vanishing gradients.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(lookback, n_features))\n",
    "    x = tf.keras.layers.SimpleRNN(HIDDEN_UNITS, return_sequences=False)(inputs)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT)(x)\n",
    "    outputs = tf.keras.layers.Dense(horizon)(x)\n",
    "    return tf.keras.Model(inputs, outputs, name=\"SimpleRNN_Forecaster\")\n",
    "\n",
    "\n",
    "def build_lstm(\n",
    "    lookback: int,\n",
    "    n_features: int,\n",
    "    horizon: int,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    LSTM: handles longer-range dependencies via gates (input/forget/output).\n",
    "\n",
    "    Why (SCM):\n",
    "    - Captures lag effects (promo lead/lag, weekly/monthly patterns).\n",
    "    - More robust than vanilla RNN for multi-week lookbacks.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(lookback, n_features))\n",
    "    x = tf.keras.layers.LSTM(HIDDEN_UNITS, return_sequences=False)(inputs)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT)(x)\n",
    "    outputs = tf.keras.layers.Dense(horizon)(x)\n",
    "    return tf.keras.Model(inputs, outputs, name=\"LSTM_Forecaster\")\n",
    "\n",
    "\n",
    "def build_gru(\n",
    "    lookback: int,\n",
    "    n_features: int,\n",
    "    horizon: int,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    GRU: similar to LSTM with fewer gates (often faster, fewer parameters).\n",
    "\n",
    "    Why:\n",
    "    - Often a strong default when you want recurrent power with lower complexity.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(lookback, n_features))\n",
    "    x = tf.keras.layers.GRU(HIDDEN_UNITS, return_sequences=False)(inputs)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT)(x)\n",
    "    outputs = tf.keras.layers.Dense(horizon)(x)\n",
    "    return tf.keras.Model(inputs, outputs, name=\"GRU_Forecaster\")\n",
    "\n",
    "\n",
    "def compile_model(model: tf.keras.Model) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Compile with MSE loss and MAE metric.\n",
    "\n",
    "    Why:\n",
    "    - MSE is a stable regression objective.\n",
    "    - MAE is interpretable; we compute MAPE/Bias separately for business reporting.\n",
    "    \"\"\"\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=\"mse\",\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c49bfe-bee5-4a85-a3bc-c6e16175cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ==== 6. TRAINING AND EVALUATION ==============================================\n",
    "# =============================================================================\n",
    "\n",
    "def fit_model(\n",
    "    model: tf.keras.Model,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    ") -> tf.keras.callbacks.History:\n",
    "    \"\"\"\n",
    "    Train with early stopping.\n",
    "\n",
    "    Why: Prevents overfitting and matches real forecasting practice.\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_business_metrics(\n",
    "    model: tf.keras.Model,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    service_level: float,\n",
    "    lead_time_days: int,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute business-facing metrics on the test set.\n",
    "\n",
    "    Note: y_test and predictions are multi-step. We evaluate:\n",
    "    - overall MAPE across all horizon points\n",
    "    - bias across all horizon points\n",
    "    - service-level impact proxy using residual variability\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "    # Flatten across horizon for simple reporting\n",
    "    y_true_flat = y_test.reshape(-1)\n",
    "    y_pred_flat = y_pred.reshape(-1)\n",
    "\n",
    "    out = {\n",
    "        \"MAPE_%\": mape(y_true_flat, y_pred_flat),\n",
    "        \"Bias_%\": bias_pct(y_true_flat, y_pred_flat),\n",
    "    }\n",
    "    out.update(service_level_impact_proxy(y_true_flat, y_pred_flat, service_level, lead_time_days))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b22c01d-93d9-4d12-b256-71f85b5b84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ==== 7. PROGRESSIVE EXAMPLES (IN ORDER) ======================================\n",
    "# =============================================================================\n",
    "\n",
    "def run_example_1_single_series_mlp() -> None:\n",
    "    \"\"\"\n",
    "    Example 1: Single SKU-Location series, MLP baseline.\n",
    "\n",
    "    Why this example first:\n",
    "    - minimal complexity\n",
    "    - shows the end-to-end pipeline (window -> train -> metrics)\n",
    "    \"\"\"\n",
    "    df = make_synthetic_sku_location_data(n_days=365, sku_id=\"SKU_001\", location_id=\"LOC_A\")\n",
    "\n",
    "    windows = make_supervised_windows(df, LOOKBACK, HORIZON, TARGET_COL, COVARIATE_COLS)\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = train_val_test_split(\n",
    "        windows.X, windows.y, TRAIN_RATIO, VAL_RATIO\n",
    "    )\n",
    "\n",
    "    # Standardize features (time series safe: fit on train only)\n",
    "    # We standardize per-feature across the lookback dimension by reshaping.\n",
    "    n_features = X_train.shape[-1]\n",
    "    X_train_2d = X_train.reshape(-1, n_features)  # [samples*lookback, features]\n",
    "    X_val_2d = X_val.reshape(-1, n_features)\n",
    "    X_test_2d = X_test.reshape(-1, n_features)\n",
    "\n",
    "    X_train_2d, X_val_2d, X_test_2d, _, _ = standardize_train_only(X_train_2d, X_val_2d, X_test_2d)\n",
    "\n",
    "    X_train = X_train_2d.reshape(-1, LOOKBACK, n_features)\n",
    "    X_val = X_val_2d.reshape(-1, LOOKBACK, n_features)\n",
    "    X_test = X_test_2d.reshape(-1, LOOKBACK, n_features)\n",
    "\n",
    "    model = compile_model(build_mlp(LOOKBACK, n_features, HORIZON))\n",
    "    fit_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    metrics = evaluate_business_metrics(model, X_test, y_test, SERVICE_LEVEL, LEAD_TIME_DAYS)\n",
    "    print(\"\\n[Example 1: MLP Single Series] Metrics\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"- {k}: {v:.4f}\" if isinstance(v, float) else f\"- {k}: {v}\")\n",
    "\n",
    "\n",
    "def run_example_2_single_series_lstm_vs_gru() -> None:\n",
    "    \"\"\"\n",
    "    Example 2: Same series, compare LSTM vs GRU.\n",
    "\n",
    "    Why:\n",
    "    - introduces recurrence and memory\n",
    "    - highlights a common practical trade-off: LSTM robustness vs GRU speed\n",
    "    \"\"\"\n",
    "    df = make_synthetic_sku_location_data(n_days=365, sku_id=\"SKU_001\", location_id=\"LOC_A\")\n",
    "\n",
    "    windows = make_supervised_windows(df, LOOKBACK, HORIZON, TARGET_COL, COVARIATE_COLS)\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = train_val_test_split(\n",
    "        windows.X, windows.y, TRAIN_RATIO, VAL_RATIO\n",
    "    )\n",
    "\n",
    "    n_features = X_train.shape[-1]\n",
    "    X_train_2d = X_train.reshape(-1, n_features)\n",
    "    X_val_2d = X_val.reshape(-1, n_features)\n",
    "    X_test_2d = X_test.reshape(-1, n_features)\n",
    "    X_train_2d, X_val_2d, X_test_2d, _, _ = standardize_train_only(X_train_2d, X_val_2d, X_test_2d)\n",
    "\n",
    "    X_train = X_train_2d.reshape(-1, LOOKBACK, n_features)\n",
    "    X_val = X_val_2d.reshape(-1, LOOKBACK, n_features)\n",
    "    X_test = X_test_2d.reshape(-1, LOOKBACK, n_features)\n",
    "\n",
    "    for builder in [build_lstm, build_gru]:\n",
    "        model = compile_model(builder(LOOKBACK, n_features, HORIZON))\n",
    "        fit_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "        metrics = evaluate_business_metrics(model, X_test, y_test, SERVICE_LEVEL, LEAD_TIME_DAYS)\n",
    "        print(f\"\\n[Example 2: {model.name}] Metrics\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"- {k}: {v:.4f}\" if isinstance(v, float) else f\"- {k}: {v}\")\n",
    "\n",
    "\n",
    "def run_example_3_multivariate_and_multi_level(level: str = \"sku_loc\") -> None:\n",
    "    \"\"\"\n",
    "    Example 3: Multi-entity dataset + aggregation level.\n",
    "\n",
    "    Why:\n",
    "    - introduces realistic SCM structure (many SKUs/locations)\n",
    "    - teaches aggregation-level forecasting choices\n",
    "\n",
    "    Note:\n",
    "    - For clarity, we train on ONE aggregated series (the first group).\n",
    "    - Extension task: loop through all groups and train per series, or build a\n",
    "      global model with embeddings for SKU/LOC.\n",
    "    \"\"\"\n",
    "    df_raw = make_multi_entity_dataset()\n",
    "    df_agg = aggregate_demand(df_raw, level=level)\n",
    "\n",
    "    # Pick one series group to keep runtime small\n",
    "    group_cols = [DATE_COL]\n",
    "    if level == \"sku_loc\":\n",
    "        group_cols += [SKU_COL, LOC_COL]\n",
    "    elif level == \"sku\":\n",
    "        group_cols += [SKU_COL]\n",
    "    elif level == \"loc\":\n",
    "        group_cols += [LOC_COL]\n",
    "    elif level == \"total\":\n",
    "        group_cols += []\n",
    "    else:\n",
    "        raise ValueError(level)\n",
    "\n",
    "    if level == \"total\":\n",
    "        df_series = df_agg.sort_values(DATE_COL)\n",
    "        series_name = \"TOTAL\"\n",
    "    else:\n",
    "        key_cols = group_cols[1:]\n",
    "        first_key = df_agg[key_cols].drop_duplicates().iloc[0].to_dict()\n",
    "        mask = np.ones(len(df_agg), dtype=bool)\n",
    "        for k, v in first_key.items():\n",
    "            mask &= (df_agg[k] == v)\n",
    "        df_series = df_agg[mask].sort_values(DATE_COL)\n",
    "        series_name = \" | \".join([f\"{k}={v}\" for k, v in first_key.items()])\n",
    "\n",
    "    windows = make_supervised_windows(df_series, LOOKBACK, HORIZON, TARGET_COL, COVARIATE_COLS)\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = train_val_test_split(\n",
    "        windows.X, windows.y, TRAIN_RATIO, VAL_RATIO\n",
    "    )\n",
    "\n",
    "    n_features = X_train.shape[-1]\n",
    "    X_train_2d = X_train.reshape(-1, n_features)\n",
    "    X_val_2d = X_val.reshape(-1, n_features)\n",
    "    X_test_2d = X_test.reshape(-1, n_features)\n",
    "    X_train_2d, X_val_2d, X_test_2d, _, _ = standardize_train_only(X_train_2d, X_val_2d, X_test_2d)\n",
    "\n",
    "    X_train = X_train_2d.reshape(-1, LOOKBACK, n_features)\n",
    "    X_val = X_val_2d.reshape(-1, LOOKBACK, n_features)\n",
    "    X_test = X_test_2d.reshape(-1, LOOKBACK, n_features)\n",
    "\n",
    "    model = compile_model(build_gru(LOOKBACK, n_features, HORIZON))\n",
    "    fit_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    metrics = evaluate_business_metrics(model, X_test, y_test, SERVICE_LEVEL, LEAD_TIME_DAYS)\n",
    "    print(f\"\\n[Example 3: Multi-level={level} | Series={series_name} | Model=GRU] Metrics\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"- {k}: {v:.4f}\" if isinstance(v, float) else f\"- {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d731427f-f611-4149-bd3b-b65993eaff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ==== 8. FORECAST HORIZON TRADE-OFFS (SHORT VS LONG) ==========================\n",
    "# =============================================================================\n",
    "\n",
    "def horizon_tradeoff_note() -> None:\n",
    "    \"\"\"\n",
    "    A small conceptual note printed to console.\n",
    "\n",
    "    Why:\n",
    "    - Short horizon: usually higher accuracy, operational replenishment decisions.\n",
    "    - Long horizon: supports S&OP/capacity planning but has higher uncertainty.\n",
    "    \"\"\"\n",
    "    print(\"\\n[Horizon Trade-off Note]\")\n",
    "    print(\"- Short horizon (e.g., 1-7 days): better accuracy, tactical decisions.\")\n",
    "    print(\"- Long horizon (e.g., 8-56 days): more uncertainty, strategic planning.\")\n",
    "    print(\"- In DL, longer horizons often benefit from more covariates and stronger regularization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02be4c-9951-47c6-b7e3-11e5b2ff55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ==== 9. TRY-YOURSELF TASKS (WITH OPTIONAL SOLUTIONS) =========================\n",
    "# =============================================================================\n",
    "\n",
    "# TODO 1: Change HORIZON from 7 to 14 and compare MAPE/Bias and implied safety stock.\n",
    "# TODO 2: Remove covariates (set COVARIATE_COLS = []) and measure accuracy drop.\n",
    "# TODO 3: Increase LOOKBACK to 56. Does LSTM/GRU improve more than MLP?\n",
    "# TODO 4: Change SERVICE_LEVEL from 0.95 to 0.99 and observe safety stock impact.\n",
    "\n",
    "# --- Optional solution hint (keep commented for learners) ---------------------\n",
    "# SOLUTION IDEA:\n",
    "# - Longer horizon usually increases residual_std -> increases implied safety stock.\n",
    "# - Covariates often reduce bias during promo/holiday periods (less systematic error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1580e26-9d7f-4cd2-8ebe-655472a9fd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time Series Demand Forecasting with DL (TensorFlow) demos...\n",
      "\n",
      "[Horizon Trade-off Note]\n",
      "- Short horizon (e.g., 1-7 days): better accuracy, tactical decisions.\n",
      "- Long horizon (e.g., 8-56 days): more uncertainty, strategic planning.\n",
      "- In DL, longer horizons often benefit from more covariates and stronger regularization.\n",
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 3175.6743 - mae: 55.4011 - val_loss: 2578.5620 - val_mae: 49.7953\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3141.2490 - mae: 55.0890 - val_loss: 2535.3364 - val_mae: 49.3598\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3104.4492 - mae: 54.7567 - val_loss: 2486.9299 - val_mae: 48.8663\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3062.9500 - mae: 54.3711 - val_loss: 2430.9097 - val_mae: 48.2868\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3019.8132 - mae: 53.9717 - val_loss: 2364.1880 - val_mae: 47.5854\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2967.3140 - mae: 53.4754 - val_loss: 2283.3848 - val_mae: 46.7193\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2899.3621 - mae: 52.8322 - val_loss: 2184.1348 - val_mae: 45.6305\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2826.7356 - mae: 52.1412 - val_loss: 2061.6772 - val_mae: 44.2472\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2726.5334 - mae: 51.1565 - val_loss: 1910.4009 - val_mae: 42.4705\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2606.9070 - mae: 49.9541 - val_loss: 1727.4691 - val_mae: 40.2106\n",
      "\n",
      "[Example 1: MLP Single Series] Metrics\n",
      "- MAPE_%: 84.9157\n",
      "- Bias_%: -85.6181\n",
      "- residual_std: 12.1038\n",
      "- implied_safety_stock_units: 52.6791\n",
      "- service_level_target: 0.9500\n",
      "- lead_time_days: 7.0000\n",
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - loss: 3143.5234 - mae: 54.9347 - val_loss: 3051.5388 - val_mae: 53.9633\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3137.5315 - mae: 54.8798 - val_loss: 3040.0813 - val_mae: 53.8573\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3129.3276 - mae: 54.8057 - val_loss: 3025.8132 - val_mae: 53.7246\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3115.6040 - mae: 54.6804 - val_loss: 3006.0093 - val_mae: 53.5397\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3091.6333 - mae: 54.4607 - val_loss: 2979.1804 - val_mae: 53.2881\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3045.3916 - mae: 54.0384 - val_loss: 2945.8826 - val_mae: 52.9740\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2980.7190 - mae: 53.4375 - val_loss: 2904.5710 - val_mae: 52.5816\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2908.0569 - mae: 52.7477 - val_loss: 2838.4690 - val_mae: 51.9483\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2828.7136 - mae: 51.9846 - val_loss: 2717.3594 - val_mae: 50.7682\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2756.8501 - mae: 51.2860 - val_loss: 2634.0017 - val_mae: 49.9373\n",
      "\n",
      "[Example 2: LSTM_Forecaster] Metrics\n",
      "- MAPE_%: 92.4555\n",
      "- Bias_%: -92.7394\n",
      "- residual_std: 10.6282\n",
      "- implied_safety_stock_units: 46.2569\n",
      "- service_level_target: 0.9500\n",
      "- lead_time_days: 7.0000\n",
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - loss: 3143.2651 - mae: 54.9325 - val_loss: 3030.7583 - val_mae: 53.7694\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3138.7864 - mae: 54.8916 - val_loss: 3021.8215 - val_mae: 53.6872\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3133.7241 - mae: 54.8456 - val_loss: 3012.4492 - val_mae: 53.6008\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3128.1077 - mae: 54.7954 - val_loss: 3001.5698 - val_mae: 53.5001\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3120.8223 - mae: 54.7286 - val_loss: 2987.3767 - val_mae: 53.3681\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3109.1318 - mae: 54.6203 - val_loss: 2966.4419 - val_mae: 53.1723\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3090.2339 - mae: 54.4466 - val_loss: 2931.3843 - val_mae: 52.8423\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3052.4890 - mae: 54.0986 - val_loss: 2863.2939 - val_mae: 52.1941\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2959.8906 - mae: 53.2309 - val_loss: 2737.5918 - val_mae: 50.9725\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2779.9561 - mae: 51.5102 - val_loss: 2600.9321 - val_mae: 49.6077\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000284D973DC60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000284D973DC60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "[Example 2: GRU_Forecaster] Metrics\n",
      "- MAPE_%: 91.8099\n",
      "- Bias_%: -92.1139\n",
      "- residual_std: 10.6215\n",
      "- implied_safety_stock_units: 46.2275\n",
      "- service_level_target: 0.9500\n",
      "- lead_time_days: 7.0000\n",
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - loss: 3095.1658 - mae: 54.7046 - val_loss: 2913.6040 - val_mae: 52.9219\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3090.1006 - mae: 54.6587 - val_loss: 2905.2944 - val_mae: 52.8438\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3085.2229 - mae: 54.6137 - val_loss: 2895.2883 - val_mae: 52.7494\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3078.9998 - mae: 54.5564 - val_loss: 2881.9253 - val_mae: 52.6227\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3072.2729 - mae: 54.4933 - val_loss: 2862.2871 - val_mae: 52.4356\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3059.8750 - mae: 54.3802 - val_loss: 2830.5852 - val_mae: 52.1318\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3036.2959 - mae: 54.1587 - val_loss: 2775.2322 - val_mae: 51.5966\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2984.2310 - mae: 53.6698 - val_loss: 2686.9268 - val_mae: 50.7304\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2854.3479 - mae: 52.4384 - val_loss: 2582.6426 - val_mae: 49.6866\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2689.5854 - mae: 50.8505 - val_loss: 2489.3015 - val_mae: 48.7330\n",
      "\n",
      "[Example 3: Multi-level=sku_loc | Series=sku_id=SKU_001 | location_id=LOC_A | Model=GRU] Metrics\n",
      "- MAPE_%: 91.5710\n",
      "- Bias_%: -91.8248\n",
      "- residual_std: 9.6101\n",
      "- implied_safety_stock_units: 41.8255\n",
      "- service_level_target: 0.9500\n",
      "- lead_time_days: 7.0000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ==== 10. MAIN: RUN DEMOS / SANITY CHECKS =====================================\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running Time Series Demand Forecasting with DL (TensorFlow) demos...\")\n",
    "\n",
    "    horizon_tradeoff_note()\n",
    "\n",
    "    # Example 1: MLP baseline\n",
    "    run_example_1_single_series_mlp()\n",
    "\n",
    "    # Example 2: LSTM vs GRU comparison\n",
    "    run_example_2_single_series_lstm_vs_gru()\n",
    "\n",
    "    # Example 3: Multi-level aggregation (try: 'sku_loc', 'sku', 'loc', 'total')\n",
    "    run_example_3_multivariate_and_multi_level(level=\"sku_loc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
