{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ef0ac5-674b-4eae-8e5e-1f85ceb60235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# File: seq2seq_multi_step_demand_forecasting.py\n",
    "# Topic: Sequence-to-Sequence (Encoder-Decoder) for multi-step demand forecasting\n",
    "# Input: Panel time series (sku_id, location_id, date, demand) + optional metadata\n",
    "# Output: H-step demand forecast curve + replenishment suggestions (ROP / order qty)\n",
    "# =============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ad9f2a-26c1-431e-af6e-b9d382191d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 0. TOP-LEVEL CONFIG AND CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Time-series windowing\n",
    "LOOKBACK_DAYS = 28          # encoder length (history)\n",
    "FORECAST_HORIZON = 14       # decoder length (future curve)\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Model\n",
    "ENCODER_UNITS = 64\n",
    "DECODER_UNITS = 64\n",
    "DROPOUT = 0.1\n",
    "EMBED_DIM = 16\n",
    "\n",
    "# Replenishment / Inventory policy (toy but realistic structure)\n",
    "SERVICE_LEVEL = 0.95        # used for safety stock approximation\n",
    "REVIEW_PERIOD_DAYS = 1      # daily review policy (s, S) simplified\n",
    "MIN_ORDER_QTY = 0           # can set to supplier MOQ if you have one\n",
    "\n",
    "# Column names (keep consistent across course files)\n",
    "COL_DATE = \"date\"\n",
    "COL_SKU = \"sku_id\"\n",
    "COL_LOC = \"location_id\"\n",
    "COL_DEMAND = \"demand\"\n",
    "\n",
    "# Reproducibility\n",
    "tf.keras.utils.set_random_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297a58e2-ce4f-4dcd-a046-54f946d30363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. UTILITIES: DATA SIMULATION (FOR A SELF-CONTAINED DEMO)\n",
    "# =============================================================================\n",
    "\n",
    "def simulate_sku_location_demand(\n",
    "    n_skus: int = 50,\n",
    "    n_locations: int = 3,\n",
    "    n_days: int = 365,\n",
    "    cold_start_ratio: float = 0.2,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a realistic-enough panel time series with:\n",
    "    - weekday seasonality\n",
    "    - trend\n",
    "    - random promo pulses\n",
    "    - SKU/Location effects\n",
    "    - cold-start SKUs that appear later in the calendar\n",
    "\n",
    "    Why this matters:\n",
    "    - Forecasting in SCM is rarely \"one series\"; it's many series (SKU x location).\n",
    "    - Cold-start is common (new products, new DCs, assortment changes).\n",
    "    \"\"\"\n",
    "    start = pd.Timestamp(\"2024-01-01\")\n",
    "    dates = pd.date_range(start, periods=n_days, freq=\"D\")\n",
    "\n",
    "    sku_ids = [f\"SKU_{i:04d}\" for i in range(n_skus)]\n",
    "    loc_ids = [f\"LOC_{j:02d}\" for j in range(n_locations)]\n",
    "\n",
    "    # Which SKUs are cold-start: they start selling later\n",
    "    n_cold = int(round(n_skus * cold_start_ratio))\n",
    "    cold_skus = set(np.random.choice(sku_ids, size=n_cold, replace=False))\n",
    "\n",
    "    rows = []\n",
    "    for sku in sku_ids:\n",
    "        # SKU baseline and volatility\n",
    "        sku_level = np.random.uniform(5, 60)\n",
    "        sku_noise = np.random.uniform(0.8, 2.5)\n",
    "\n",
    "        # Cold-start: sales begin later (e.g., after product launch)\n",
    "        launch_offset = np.random.randint(60, 200) if sku in cold_skus else 0\n",
    "\n",
    "        for loc in loc_ids:\n",
    "            loc_multiplier = np.random.uniform(0.7, 1.3)\n",
    "\n",
    "            # Promo pulses: rare multiplicative spikes\n",
    "            promo_days = set(np.random.choice(np.arange(n_days), size=n_days // 25, replace=False))\n",
    "\n",
    "            for t, d in enumerate(dates):\n",
    "                if t < launch_offset:\n",
    "                    demand = 0.0\n",
    "                else:\n",
    "                    weekday = d.dayofweek  # 0=Mon ... 6=Sun\n",
    "                    weekly_season = 1.0 + 0.2 * math.sin(2 * math.pi * weekday / 7)\n",
    "\n",
    "                    trend = 1.0 + 0.0008 * t\n",
    "                    promo = 1.0 + (np.random.uniform(0.3, 1.2) if t in promo_days else 0.0)\n",
    "\n",
    "                    mean = sku_level * loc_multiplier * weekly_season * trend * promo\n",
    "                    demand = np.random.poisson(lam=max(mean, 0.1)) + np.random.normal(0, sku_noise)\n",
    "\n",
    "                    # Demand must be non-negative for most retail/parts use-cases\n",
    "                    demand = float(max(demand, 0.0))\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        COL_DATE: d,\n",
    "                        COL_SKU: sku,\n",
    "                        COL_LOC: loc,\n",
    "                        COL_DEMAND: demand,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values([COL_SKU, COL_LOC, COL_DATE]).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1225fc-7d39-4cd8-9676-39207cfcba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. FEATURE ENGINEERING & WINDOWING\n",
    "# =============================================================================\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add simple calendar features.\n",
    "\n",
    "    Why this matters:\n",
    "    - Seq2Seq can learn seasonality implicitly, but giving explicit calendar\n",
    "      signals often improves generalization (especially with short histories).\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[\"dow\"] = out[COL_DATE].dt.dayofweek.astype(np.int32)\n",
    "    out[\"month\"] = out[COL_DATE].dt.month.astype(np.int32)\n",
    "    out[\"is_weekend\"] = (out[\"dow\"] >= 5).astype(np.int32)\n",
    "    return out\n",
    "\n",
    "\n",
    "def fit_scaler_per_series(df: pd.DataFrame) -> Dict[Tuple[str, str], Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Per-(SKU,Location) scaling using mean/std.\n",
    "\n",
    "    Why per-series:\n",
    "    - SKU demand scales differ massively (fast vs slow movers).\n",
    "    - Global scaling often underfits slow movers and overfits fast movers.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    grp = df.groupby([COL_SKU, COL_LOC])[COL_DEMAND]\n",
    "    for key, s in grp:\n",
    "        mu = float(s.mean())\n",
    "        sigma = float(s.std(ddof=0))\n",
    "        sigma = sigma if sigma > 1e-6 else 1.0\n",
    "        stats[key] = (mu, sigma)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def apply_scaler(\n",
    "    df: pd.DataFrame,\n",
    "    stats: Dict[Tuple[str, str], Tuple[float, float]],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply per-series scaling. Result column: demand_scaled.\n",
    "\n",
    "    Note:\n",
    "    - For cold-start windows with all zeros, mean/std are stable.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    mu = []\n",
    "    sig = []\n",
    "    for sku, loc in zip(out[COL_SKU], out[COL_LOC]):\n",
    "        m, s = stats[(sku, loc)]\n",
    "        mu.append(m)\n",
    "        sig.append(s)\n",
    "    out[\"mu\"] = np.array(mu, dtype=np.float32)\n",
    "    out[\"sigma\"] = np.array(sig, dtype=np.float32)\n",
    "    out[\"demand_scaled\"] = (out[COL_DEMAND].astype(np.float32) - out[\"mu\"]) / out[\"sigma\"]\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_id_maps(df: pd.DataFrame) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"Create integer IDs for embeddings.\"\"\"\n",
    "    sku_map = {v: i for i, v in enumerate(sorted(df[COL_SKU].unique()))}\n",
    "    loc_map = {v: i for i, v in enumerate(sorted(df[COL_LOC].unique()))}\n",
    "    return sku_map, loc_map\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WindowedDataset:\n",
    "    \"\"\"\n",
    "    Holds arrays for a teacher-forced seq2seq training setup.\n",
    "\n",
    "    Encoder inputs:\n",
    "      - past demand_scaled\n",
    "      - time features (dow, month, weekend)\n",
    "      - embeddings IDs (sku_id, location_id) as separate inputs\n",
    "\n",
    "    Decoder inputs (teacher forcing):\n",
    "      - previous true demand_scaled values, shifted right by 1 step\n",
    "      - future time features (optional but helpful)\n",
    "\n",
    "    Decoder target:\n",
    "      - true demand_scaled for horizon\n",
    "    \"\"\"\n",
    "    enc_demand: np.ndarray\n",
    "    enc_time: np.ndarray\n",
    "    dec_in_demand: np.ndarray\n",
    "    dec_time: np.ndarray\n",
    "    y: np.ndarray\n",
    "    sku_id: np.ndarray\n",
    "    loc_id: np.ndarray\n",
    "\n",
    "\n",
    "def make_seq2seq_windows(\n",
    "    df: pd.DataFrame,\n",
    "    sku_map: Dict[str, int],\n",
    "    loc_map: Dict[str, int],\n",
    "    lookback: int = LOOKBACK_DAYS,\n",
    "    horizon: int = FORECAST_HORIZON,\n",
    "    step: int = 1,\n",
    ") -> WindowedDataset:\n",
    "    \"\"\"\n",
    "    Convert panel data into (encoder_window, decoder_window) samples.\n",
    "\n",
    "    Key design choices:\n",
    "    - Teacher forcing requires decoder inputs. We feed the *previous* true value.\n",
    "    - We include time features on both encoder and decoder sides.\n",
    "    - We attach sku_id and location_id to every sample (embeddings).\n",
    "\n",
    "    Cold-start handling:\n",
    "    - Cold-start shows up naturally as early sequences with many zeros.\n",
    "    - Embeddings let the model learn \"SKU identity priors\" and \"location priors\"\n",
    "      that can help even when history is short.\n",
    "    \"\"\"\n",
    "    enc_demand_list, enc_time_list = [], []\n",
    "    dec_in_demand_list, dec_time_list = [], []\n",
    "    y_list = []\n",
    "    sku_id_list, loc_id_list = [], []\n",
    "\n",
    "    # Ensure correct ordering\n",
    "    df = df.sort_values([COL_SKU, COL_LOC, COL_DATE]).reset_index(drop=True)\n",
    "\n",
    "    # Time features we will feed to model\n",
    "    time_cols = [\"dow\", \"month\", \"is_weekend\"]\n",
    "\n",
    "    for (sku, loc), g in df.groupby([COL_SKU, COL_LOC], sort=False):\n",
    "        g = g.reset_index(drop=True)\n",
    "\n",
    "        demand = g[\"demand_scaled\"].to_numpy(dtype=np.float32)\n",
    "        time_feat = g[time_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "        n = len(g)\n",
    "        # windows: [t-lookback, t) -> forecast [t, t+horizon)\n",
    "        for t in range(lookback, n - horizon + 1, step):\n",
    "            enc_d = demand[t - lookback : t]                          # (lookback,)\n",
    "            enc_t = time_feat[t - lookback : t]                       # (lookback, time_dim)\n",
    "\n",
    "            y = demand[t : t + horizon]                               # (horizon,)\n",
    "            dec_t = time_feat[t : t + horizon]                        # (horizon, time_dim)\n",
    "\n",
    "            # Decoder input = shifted right by 1:\n",
    "            #   at step0 we provide last encoder value (or 0) as \"previous demand\"\n",
    "            # This \"anchors\" the decoder to the last observed level.\n",
    "            last_enc = enc_d[-1]\n",
    "            dec_in = np.concatenate([[last_enc], y[:-1]], axis=0)      # (horizon,)\n",
    "\n",
    "            enc_demand_list.append(enc_d[:, None])                     # add feature dim => (lookback, 1)\n",
    "            enc_time_list.append(enc_t)\n",
    "            dec_in_demand_list.append(dec_in[:, None])                 # (horizon, 1)\n",
    "            dec_time_list.append(dec_t)\n",
    "            y_list.append(y[:, None])                                  # (horizon, 1)\n",
    "\n",
    "            sku_id_list.append(sku_map[sku])\n",
    "            loc_id_list.append(loc_map[loc])\n",
    "\n",
    "    return WindowedDataset(\n",
    "        enc_demand=np.array(enc_demand_list, dtype=np.float32),\n",
    "        enc_time=np.array(enc_time_list, dtype=np.float32),\n",
    "        dec_in_demand=np.array(dec_in_demand_list, dtype=np.float32),\n",
    "        dec_time=np.array(dec_time_list, dtype=np.float32),\n",
    "        y=np.array(y_list, dtype=np.float32),\n",
    "        sku_id=np.array(sku_id_list, dtype=np.int32),\n",
    "        loc_id=np.array(loc_id_list, dtype=np.int32),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_val_split(w: WindowedDataset, val_ratio: float = 0.2) -> Tuple[WindowedDataset, WindowedDataset]:\n",
    "    \"\"\"Simple random split for demo purposes.\"\"\"\n",
    "    n = w.y.shape[0]\n",
    "    idx = np.arange(n)\n",
    "    np.random.shuffle(idx)\n",
    "    n_val = int(round(n * val_ratio))\n",
    "    val_idx = idx[:n_val]\n",
    "    trn_idx = idx[n_val:]\n",
    "\n",
    "    def take(arr, ids):\n",
    "        return arr[ids]\n",
    "\n",
    "    trn = WindowedDataset(\n",
    "        enc_demand=take(w.enc_demand, trn_idx),\n",
    "        enc_time=take(w.enc_time, trn_idx),\n",
    "        dec_in_demand=take(w.dec_in_demand, trn_idx),\n",
    "        dec_time=take(w.dec_time, trn_idx),\n",
    "        y=take(w.y, trn_idx),\n",
    "        sku_id=take(w.sku_id, trn_idx),\n",
    "        loc_id=take(w.loc_id, trn_idx),\n",
    "    )\n",
    "    val = WindowedDataset(\n",
    "        enc_demand=take(w.enc_demand, val_idx),\n",
    "        enc_time=take(w.enc_time, val_idx),\n",
    "        dec_in_demand=take(w.dec_in_demand, val_idx),\n",
    "        dec_time=take(w.dec_time, val_idx),\n",
    "        y=take(w.y, val_idx),\n",
    "        sku_id=take(w.sku_id, val_idx),\n",
    "        loc_id=take(w.loc_id, val_idx),\n",
    "    )\n",
    "    return trn, val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c7b987-27ea-460b-9982-1f1f0e4fded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. MODEL: ENCODER-DECODER (TEACHER FORCING TRAINING GRAPH)\n",
    "# =============================================================================\n",
    "\n",
    "def build_seq2seq_model(\n",
    "    n_skus: int,\n",
    "    n_locations: int,\n",
    "    time_dim: int = 3,\n",
    "    embed_dim: int = EMBED_DIM,\n",
    "    enc_units: int = ENCODER_UNITS,\n",
    "    dec_units: int = DECODER_UNITS,\n",
    "    dropout: float = DROPOUT,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Encoder-Decoder with LSTM.\n",
    "\n",
    "    Inputs:\n",
    "      - encoder_demand: (B, lookback, 1)\n",
    "      - encoder_time:   (B, lookback, time_dim)\n",
    "      - decoder_demand: (B, horizon, 1)    teacher-forced \"previous y\"\n",
    "      - decoder_time:   (B, horizon, time_dim)\n",
    "      - sku_id:         (B,)\n",
    "      - loc_id:         (B,)\n",
    "\n",
    "    Output:\n",
    "      - y_hat: (B, horizon, 1) predicted demand_scaled curve\n",
    "\n",
    "    Why embeddings:\n",
    "      - Cold-start SKUs: embeddings provide a learnable prior per SKU/Location.\n",
    "      - It’s a pragmatic compromise when you do not have rich product attributes.\n",
    "    \"\"\"\n",
    "    # ----- Inputs\n",
    "    enc_demand_in = tf.keras.Input(shape=(None, 1), name=\"enc_demand\")\n",
    "    enc_time_in = tf.keras.Input(shape=(None, time_dim), name=\"enc_time\")\n",
    "    dec_demand_in = tf.keras.Input(shape=(None, 1), name=\"dec_demand_in\")\n",
    "    dec_time_in = tf.keras.Input(shape=(None, time_dim), name=\"dec_time\")\n",
    "\n",
    "    sku_id_in = tf.keras.Input(shape=(), dtype=tf.int32, name=\"sku_id\")\n",
    "    loc_id_in = tf.keras.Input(shape=(), dtype=tf.int32, name=\"loc_id\")\n",
    "\n",
    "    # ----- Embeddings\n",
    "    sku_emb = tf.keras.layers.Embedding(n_skus, embed_dim, name=\"sku_emb\")(sku_id_in)   # (B, embed_dim)\n",
    "    loc_emb = tf.keras.layers.Embedding(n_locations, embed_dim, name=\"loc_emb\")(loc_id_in)\n",
    "\n",
    "    # Combine embeddings -> one context vector\n",
    "    context = tf.keras.layers.Concatenate(name=\"id_context\")([sku_emb, loc_emb])        # (B, 2*embed_dim)\n",
    "    context = tf.keras.layers.Dropout(dropout)(context)\n",
    "\n",
    "    # Repeat context across time for encoder and decoder\n",
    "    def repeat_context(x, steps_tensor):\n",
    "        # steps_tensor is a sequence input; we repeat context to match its time dimension\n",
    "        steps = tf.shape(steps_tensor)[1]\n",
    "        return tf.tile(x[:, None, :], multiples=[1, steps, 1])\n",
    "\n",
    "    enc_ctx = tf.keras.layers.Lambda(lambda t: repeat_context(t[0], t[1]), name=\"enc_ctx\")([context, enc_demand_in])\n",
    "    dec_ctx = tf.keras.layers.Lambda(lambda t: repeat_context(t[0], t[1]), name=\"dec_ctx\")([context, dec_demand_in])\n",
    "\n",
    "    # ----- Encoder\n",
    "    enc_x = tf.keras.layers.Concatenate(name=\"enc_concat\")([enc_demand_in, enc_time_in, enc_ctx])\n",
    "    enc_x = tf.keras.layers.LayerNormalization()(enc_x)\n",
    "    enc_lstm = tf.keras.layers.LSTM(\n",
    "        enc_units,\n",
    "        return_state=True,\n",
    "        name=\"encoder_lstm\",\n",
    "        dropout=dropout,\n",
    "        recurrent_dropout=0.0,  # recurrent dropout can be slower/unstable; keep simple\n",
    "    )\n",
    "    _, enc_h, enc_c = enc_lstm(enc_x)\n",
    "\n",
    "    # ----- Decoder (teacher forcing)\n",
    "    dec_x = tf.keras.layers.Concatenate(name=\"dec_concat\")([dec_demand_in, dec_time_in, dec_ctx])\n",
    "    dec_x = tf.keras.layers.LayerNormalization()(dec_x)\n",
    "    dec_lstm = tf.keras.layers.LSTM(\n",
    "        dec_units,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "        name=\"decoder_lstm\",\n",
    "        dropout=dropout,\n",
    "        recurrent_dropout=0.0,\n",
    "    )\n",
    "    dec_seq, _, _ = dec_lstm(dec_x, initial_state=[enc_h, enc_c])\n",
    "\n",
    "    # Output projection per time step\n",
    "    y_hat = tf.keras.layers.Dense(1, name=\"y_hat\")(dec_seq)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs={\n",
    "            \"enc_demand\": enc_demand_in,\n",
    "            \"enc_time\": enc_time_in,\n",
    "            \"dec_demand_in\": dec_demand_in,\n",
    "            \"dec_time\": dec_time_in,\n",
    "            \"sku_id\": sku_id_in,\n",
    "            \"loc_id\": loc_id_in,\n",
    "        },\n",
    "        outputs=y_hat,\n",
    "        name=\"seq2seq_demand_forecaster\",\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=tf.keras.losses.Huber(),  # robust to spikes/promos\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e29376-9f7d-4658-be12-6ce104648034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. FORECASTING MODES: DIRECT (ONE-SHOT) VS RECURSIVE/ROLLING (AUTO-REGRESSIVE)\n",
    "# =============================================================================\n",
    "\n",
    "def direct_forecast(\n",
    "    model: tf.keras.Model,\n",
    "    enc_demand: np.ndarray,\n",
    "    enc_time: np.ndarray,\n",
    "    dec_time: np.ndarray,\n",
    "    sku_id: np.ndarray,\n",
    "    loc_id: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    DIRECT multi-step forecast:\n",
    "    - Provide decoder inputs (previous y) as a constant seed repeated, OR\n",
    "      use last encoder value and then zeros (naive).\n",
    "    - Model outputs full curve in one forward pass.\n",
    "\n",
    "    Why it’s called \"direct\":\n",
    "    - You predict all horizons jointly without feeding predictions back.\n",
    "\n",
    "    Trade-off:\n",
    "    - Pros: avoids error accumulation.\n",
    "    - Cons: decoder inputs are not \"true previous y\" at inference.\n",
    "    \"\"\"\n",
    "    # Seed: last observed demand from encoder (per sample)\n",
    "    last_y = enc_demand[:, -1:, :]                             # (B, 1, 1)\n",
    "    horizon = dec_time.shape[1]\n",
    "\n",
    "    # Build decoder input: last_y then zeros (simple, common baseline)\n",
    "    zeros = np.zeros((enc_demand.shape[0], horizon - 1, 1), dtype=np.float32)\n",
    "    dec_in = np.concatenate([last_y, zeros], axis=1)           # (B, H, 1)\n",
    "\n",
    "    y_hat = model.predict(\n",
    "        {\n",
    "            \"enc_demand\": enc_demand,\n",
    "            \"enc_time\": enc_time,\n",
    "            \"dec_demand_in\": dec_in,\n",
    "            \"dec_time\": dec_time,\n",
    "            \"sku_id\": sku_id,\n",
    "            \"loc_id\": loc_id,\n",
    "        },\n",
    "        verbose=0,\n",
    "    )\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def rolling_forecast(\n",
    "    model: tf.keras.Model,\n",
    "    enc_demand: np.ndarray,\n",
    "    enc_time: np.ndarray,\n",
    "    dec_time: np.ndarray,\n",
    "    sku_id: np.ndarray,\n",
    "    loc_id: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ROLLING / RECURSIVE multi-step forecast:\n",
    "    - Predict step 1\n",
    "    - Feed prediction back as previous y for step 2\n",
    "    - Repeat until horizon H\n",
    "\n",
    "    Why people use it:\n",
    "    - Closer to how teacher forcing is trained (decoder sees previous y values).\n",
    "    - Works well when short-horizon accuracy is strong and dynamics are stable.\n",
    "\n",
    "    Risk:\n",
    "    - Error accumulation: small bias at step 1 can compound by step H.\n",
    "    \"\"\"\n",
    "    b = enc_demand.shape[0]\n",
    "    h = dec_time.shape[1]\n",
    "\n",
    "    last_y = enc_demand[:, -1, :]                              # (B, 1)\n",
    "    preds = []\n",
    "\n",
    "    prev_y = last_y.copy()                                     # (B, 1)\n",
    "    for t in range(h):\n",
    "        # Construct decoder input for this step: shape (B, 1, 1)\n",
    "        dec_in_step = prev_y[:, None, :]                       # (B, 1, 1)\n",
    "        dec_time_step = dec_time[:, t:t+1, :]                  # (B, 1, time_dim)\n",
    "\n",
    "        y_hat_step = model.predict(\n",
    "            {\n",
    "                \"enc_demand\": enc_demand,\n",
    "                \"enc_time\": enc_time,\n",
    "                \"dec_demand_in\": dec_in_step,\n",
    "                \"dec_time\": dec_time_step,\n",
    "                \"sku_id\": sku_id,\n",
    "                \"loc_id\": loc_id,\n",
    "            },\n",
    "            verbose=0,\n",
    "        )                                                      # (B, 1, 1)\n",
    "\n",
    "        preds.append(y_hat_step)\n",
    "        prev_y = y_hat_step[:, 0, :]                           # feed back (B, 1)\n",
    "\n",
    "    return np.concatenate(preds, axis=1)                        # (B, H, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf5682da-4bb9-430e-84ef-da4eac742761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. TRANSLATING FORECASTS INTO REPLENISHMENT DECISIONS\n",
    "# =============================================================================\n",
    "\n",
    "def z_value(service_level: float) -> float:\n",
    "    \"\"\"\n",
    "    Approximate z-score for a service level using inverse error function.\n",
    "\n",
    "    Why this matters:\n",
    "    - For a simple safety stock approximation: SS = z * sigma_LT\n",
    "    - In production, you'd use scipy.stats.norm.ppf; we avoid extra deps here.\n",
    "    \"\"\"\n",
    "    # Convert service level to a normal quantile approximation\n",
    "    # Using approximation via inverse erf: ppf(p) = sqrt(2) * erfinv(2p - 1)\n",
    "    return math.sqrt(2) * float(tf.math.erfinv(2.0 * service_level - 1.0).numpy())\n",
    "\n",
    "\n",
    "def replenishment_from_forecast(\n",
    "    forecast_daily: np.ndarray,\n",
    "    on_hand: float,\n",
    "    on_order: float,\n",
    "    lead_time_days: int,\n",
    "    service_level: float = SERVICE_LEVEL,\n",
    "    min_order_qty: int = MIN_ORDER_QTY,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Convert an H-day forecast curve into a simple reorder suggestion.\n",
    "\n",
    "    Policy (simplified continuous review style):\n",
    "      Inventory Position (IP) = on_hand + on_order\n",
    "      Lead-time demand (LTD) = sum(forecast over lead_time_days)\n",
    "      Safety stock (SS) approximated from forecast volatility\n",
    "      Reorder point (ROP) = LTD + SS\n",
    "      Order qty = max(0, ROP - IP), optionally rounded / MOQ\n",
    "\n",
    "    Why this is useful pedagogically:\n",
    "    - Forecasts are not the end goal; decisions are.\n",
    "    - Students see how multi-step forecasts map to inventory parameters.\n",
    "    \"\"\"\n",
    "    lead_time_days = int(max(1, lead_time_days))\n",
    "    ltd = float(np.sum(forecast_daily[:lead_time_days]))\n",
    "\n",
    "    # Approximate sigma over lead time using daily std * sqrt(L)\n",
    "    daily_sigma = float(np.std(forecast_daily[:lead_time_days], ddof=0))\n",
    "    sigma_lt = daily_sigma * math.sqrt(lead_time_days)\n",
    "\n",
    "    z = z_value(service_level)\n",
    "    safety_stock = z * sigma_lt\n",
    "    reorder_point = ltd + safety_stock\n",
    "\n",
    "    inventory_position = float(on_hand + on_order)\n",
    "    raw_order = max(0.0, reorder_point - inventory_position)\n",
    "\n",
    "    # Apply MOQ / rounding if desired (keep simple for course)\n",
    "    order_qty = float(max(raw_order, float(min_order_qty))) if raw_order > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"lead_time_demand\": ltd,\n",
    "        \"daily_sigma\": daily_sigma,\n",
    "        \"safety_stock\": float(safety_stock),\n",
    "        \"reorder_point\": float(reorder_point),\n",
    "        \"inventory_position\": inventory_position,\n",
    "        \"order_qty_suggested\": order_qty,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30dcf4b-3b56-426a-8ff5-895043f020b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. PROGRESSIVE EXAMPLES (FROM SIMPLE TO REALISTIC)\n",
    "# =============================================================================\n",
    "\n",
    "def example_1_minimal_seq2seq_shapes() -> None:\n",
    "    \"\"\"\n",
    "    Example 1: Minimal sanity check of shapes only.\n",
    "\n",
    "    Why:\n",
    "    - Learners often get stuck on 3D tensors (B, T, F).\n",
    "    - This example isolates the mechanics without business complexity.\n",
    "    \"\"\"\n",
    "    b, lookback, horizon = 4, 7, 3\n",
    "    time_dim = 3\n",
    "\n",
    "    enc_demand = np.random.randn(b, lookback, 1).astype(np.float32)\n",
    "    enc_time = np.random.randn(b, lookback, time_dim).astype(np.float32)\n",
    "    dec_time = np.random.randn(b, horizon, time_dim).astype(np.float32)\n",
    "\n",
    "    sku_id = np.random.randint(0, 10, size=(b,), dtype=np.int32)\n",
    "    loc_id = np.random.randint(0, 2, size=(b,), dtype=np.int32)\n",
    "\n",
    "    model = build_seq2seq_model(n_skus=10, n_locations=2, time_dim=time_dim)\n",
    "    y_hat = direct_forecast(model, enc_demand, enc_time, dec_time, sku_id, loc_id)\n",
    "\n",
    "    print(\"Example 1 shapes\")\n",
    "    print(\"enc_demand:\", enc_demand.shape)\n",
    "    print(\"dec_time:  \", dec_time.shape)\n",
    "    print(\"y_hat:     \", y_hat.shape)\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "def example_2_train_on_simulated_panel() -> Tuple[tf.keras.Model, pd.DataFrame, Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Example 2: End-to-end training on simulated SKU-location panel.\n",
    "\n",
    "    What you learn here:\n",
    "    - How to create teacher-forced decoder inputs\n",
    "    - How seq2seq can output full demand curves\n",
    "    - How cold-start SKUs are represented\n",
    "    \"\"\"\n",
    "    df = simulate_sku_location_demand(n_skus=80, n_locations=4, n_days=420, cold_start_ratio=0.25)\n",
    "    df = add_time_features(df)\n",
    "\n",
    "    sku_map, loc_map = build_id_maps(df)\n",
    "\n",
    "    # Fit scaler on full data for demo simplicity\n",
    "    # In production, fit on TRAIN period only to avoid leakage.\n",
    "    stats = fit_scaler_per_series(df)\n",
    "    df_scaled = apply_scaler(df, stats)\n",
    "\n",
    "    w = make_seq2seq_windows(df_scaled, sku_map, loc_map, LOOKBACK_DAYS, FORECAST_HORIZON, step=2)\n",
    "    trn, val = train_val_split(w, val_ratio=0.2)\n",
    "\n",
    "    model = build_seq2seq_model(n_skus=len(sku_map), n_locations=len(loc_map), time_dim=3)\n",
    "\n",
    "    # Build tf.data (preferred for performance and cleanliness)\n",
    "    def to_ds(ws: WindowedDataset, shuffle: bool) -> tf.data.Dataset:\n",
    "        x = {\n",
    "            \"enc_demand\": ws.enc_demand,\n",
    "            \"enc_time\": ws.enc_time,\n",
    "            \"dec_demand_in\": ws.dec_in_demand,\n",
    "            \"dec_time\": ws.dec_time,\n",
    "            \"sku_id\": ws.sku_id,\n",
    "            \"loc_id\": ws.loc_id,\n",
    "        }\n",
    "        y = ws.y\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=min(50_000, y.shape[0]), seed=RANDOM_SEED)\n",
    "        return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    trn_ds = to_ds(trn, shuffle=True)\n",
    "    val_ds = to_ds(val, shuffle=False)\n",
    "\n",
    "    model.fit(trn_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)\n",
    "\n",
    "    artifacts = {\n",
    "        \"df\": df_scaled,\n",
    "        \"stats\": stats,\n",
    "        \"sku_map\": sku_map,\n",
    "        \"loc_map\": loc_map,\n",
    "    }\n",
    "    return model, df_scaled, stats, sku_map, loc_map\n",
    "\n",
    "\n",
    "def example_3_compare_direct_vs_recursive(\n",
    "    model: tf.keras.Model,\n",
    "    df_scaled: pd.DataFrame,\n",
    "    stats: Dict[Tuple[str, str], Tuple[float, float]],\n",
    "    sku_map: Dict[str, int],\n",
    "    loc_map: Dict[str, int],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Example 3: Compare DIRECT vs ROLLING forecasts for a single (SKU,LOC).\n",
    "\n",
    "    What to look for:\n",
    "    - Direct often has smoother curves\n",
    "    - Rolling can track momentum but may drift if biased early\n",
    "    \"\"\"\n",
    "    # Pick a random series\n",
    "    sku = np.random.choice(list(sku_map.keys()))\n",
    "    loc = np.random.choice(list(loc_map.keys()))\n",
    "\n",
    "    g = df_scaled[(df_scaled[COL_SKU] == sku) & (df_scaled[COL_LOC] == loc)].sort_values(COL_DATE)\n",
    "    g = g.reset_index(drop=True)\n",
    "\n",
    "    # Choose a forecast origin near the end\n",
    "    t = len(g) - FORECAST_HORIZON - 1\n",
    "    t = max(LOOKBACK_DAYS, t)\n",
    "\n",
    "    enc = g.iloc[t - LOOKBACK_DAYS : t]\n",
    "    fut = g.iloc[t : t + FORECAST_HORIZON]\n",
    "\n",
    "    enc_demand = enc[\"demand_scaled\"].to_numpy(np.float32)[None, :, None]\n",
    "    enc_time = enc[[\"dow\", \"month\", \"is_weekend\"]].to_numpy(np.float32)[None, :, :]\n",
    "    dec_time = fut[[\"dow\", \"month\", \"is_weekend\"]].to_numpy(np.float32)[None, :, :]\n",
    "\n",
    "    sku_id = np.array([sku_map[sku]], dtype=np.int32)\n",
    "    loc_id = np.array([loc_map[loc]], dtype=np.int32)\n",
    "\n",
    "    y_hat_direct = direct_forecast(model, enc_demand, enc_time, dec_time, sku_id, loc_id)[0, :, 0]\n",
    "    y_hat_roll = rolling_forecast(model, enc_demand, enc_time, dec_time, sku_id, loc_id)[0, :, 0]\n",
    "    y_true = fut[\"demand_scaled\"].to_numpy(np.float32)\n",
    "\n",
    "    # Inverse scaling back to demand units\n",
    "    mu, sigma = stats[(sku, loc)]\n",
    "    y_direct = y_hat_direct * sigma + mu\n",
    "    y_roll = y_hat_roll * sigma + mu\n",
    "    y_true_u = y_true * sigma + mu\n",
    "\n",
    "    mae_direct = float(np.mean(np.abs(y_direct - y_true_u)))\n",
    "    mae_roll = float(np.mean(np.abs(y_roll - y_true_u)))\n",
    "\n",
    "    print(\"Example 3: Direct vs Rolling\")\n",
    "    print(f\"Series: {sku} @ {loc}\")\n",
    "    print(f\"MAE Direct : {mae_direct:.2f}\")\n",
    "    print(f\"MAE Rolling: {mae_roll:.2f}\")\n",
    "    print(\"First 5 days (true / direct / rolling):\")\n",
    "    for i in range(min(5, FORECAST_HORIZON)):\n",
    "        print(f\"  D+{i+1:02d}: {y_true_u[i]:7.2f} / {y_direct[i]:7.2f} / {y_roll[i]:7.2f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "def example_4_forecast_to_replenishment(\n",
    "    model: tf.keras.Model,\n",
    "    df_scaled: pd.DataFrame,\n",
    "    stats: Dict[Tuple[str, str], Tuple[float, float]],\n",
    "    sku_map: Dict[str, int],\n",
    "    loc_map: Dict[str, int],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Example 4: Convert a forecast curve into a reorder suggestion.\n",
    "\n",
    "    This connects forecasting to a real SCM decision.\n",
    "    \"\"\"\n",
    "    sku = np.random.choice(list(sku_map.keys()))\n",
    "    loc = np.random.choice(list(loc_map.keys()))\n",
    "\n",
    "    g = df_scaled[(df_scaled[COL_SKU] == sku) & (df_scaled[COL_LOC] == loc)].sort_values(COL_DATE)\n",
    "    g = g.reset_index(drop=True)\n",
    "\n",
    "    t = len(g) - FORECAST_HORIZON - 1\n",
    "    t = max(LOOKBACK_DAYS, t)\n",
    "\n",
    "    enc = g.iloc[t - LOOKBACK_DAYS : t]\n",
    "    fut = g.iloc[t : t + FORECAST_HORIZON]\n",
    "\n",
    "    enc_demand = enc[\"demand_scaled\"].to_numpy(np.float32)[None, :, None]\n",
    "    enc_time = enc[[\"dow\", \"month\", \"is_weekend\"]].to_numpy(np.float32)[None, :, :]\n",
    "    dec_time = fut[[\"dow\", \"month\", \"is_weekend\"]].to_numpy(np.float32)[None, :, :]\n",
    "\n",
    "    sku_id = np.array([sku_map[sku]], dtype=np.int32)\n",
    "    loc_id = np.array([loc_map[loc]], dtype=np.int32)\n",
    "\n",
    "    # Use direct forecast for stability in this demo\n",
    "    y_hat_scaled = direct_forecast(model, enc_demand, enc_time, dec_time, sku_id, loc_id)[0, :, 0]\n",
    "\n",
    "    # Back to demand units\n",
    "    mu, sigma = stats[(sku, loc)]\n",
    "    forecast_units = (y_hat_scaled * sigma + mu).clip(min=0.0)\n",
    "\n",
    "    # Example inventory situation (in real life: from ERP/WMS)\n",
    "    on_hand = float(np.random.uniform(0, 200))\n",
    "    on_order = float(np.random.uniform(0, 150))\n",
    "    lead_time_days = int(np.random.choice([3, 5, 7, 10, 14]))\n",
    "\n",
    "    decision = replenishment_from_forecast(\n",
    "        forecast_daily=forecast_units,\n",
    "        on_hand=on_hand,\n",
    "        on_order=on_order,\n",
    "        lead_time_days=lead_time_days,\n",
    "        service_level=SERVICE_LEVEL,\n",
    "        min_order_qty=MIN_ORDER_QTY,\n",
    "    )\n",
    "\n",
    "    print(\"Example 4: Forecast -> Replenishment\")\n",
    "    print(f\"Series: {sku} @ {loc}\")\n",
    "    print(f\"On hand: {on_hand:.1f} | On order: {on_order:.1f} | Lead time: {lead_time_days} days\")\n",
    "    print(f\"Lead-time demand: {decision['lead_time_demand']:.2f}\")\n",
    "    print(f\"Safety stock:     {decision['safety_stock']:.2f} (service level={SERVICE_LEVEL})\")\n",
    "    print(f\"Reorder point:    {decision['reorder_point']:.2f}\")\n",
    "    print(f\"Inv position:     {decision['inventory_position']:.2f}\")\n",
    "    print(f\"Order suggested:  {decision['order_qty_suggested']:.2f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d92ee-121b-4319-a9d9-5cc5b9b9135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. TINY \"TRY YOURSELF\" TASKS (WITH OPTIONAL SOLUTIONS)\n",
    "# =============================================================================\n",
    "\n",
    "# TODO 1:\n",
    "# Change FORECAST_HORIZON from 14 to 28 and retrain.\n",
    "# Question: Does MAE improve or degrade? Why might longer horizons be harder?\n",
    "\n",
    "# TODO 2:\n",
    "# Change SERVICE_LEVEL from 0.95 to 0.99 and re-run Example 4.\n",
    "# Question: How does safety stock change? Why is the relationship non-linear?\n",
    "\n",
    "# TODO 3:\n",
    "# Switch loss from Huber() to MeanSquaredError().\n",
    "# Question: What happens with promo spikes? Which loss is more robust?\n",
    "\n",
    "# --- Optional Solution Hints (keep short, let learners experiment) ---\n",
    "# - Longer horizons often degrade because uncertainty grows and the model must\n",
    "#   learn longer-range seasonality/trend, plus cold-start becomes more severe.\n",
    "# - Service level maps to z-score; going 0.95 -> 0.99 increases z materially.\n",
    "# - MSE penalizes large errors strongly, which can overreact to promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eedab95-349b-4445-b15c-750be18c37f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Example 1 shapes\n",
      "enc_demand: (4, 7, 1)\n",
      "dec_time:   (4, 3, 3)\n",
      "y_hat:      (4, 3, 1)\n",
      "------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 47ms/step - loss: 0.2738 - mae: 0.5921 - val_loss: 0.2354 - val_mae: 0.5316\n",
      "Epoch 2/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - loss: 0.2347 - mae: 0.5298 - val_loss: 0.2253 - val_mae: 0.5115\n",
      "Epoch 3/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 46ms/step - loss: 0.2266 - mae: 0.5151 - val_loss: 0.2225 - val_mae: 0.5053\n",
      "Epoch 4/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 44ms/step - loss: 0.2236 - mae: 0.5095 - val_loss: 0.2242 - val_mae: 0.5046\n",
      "Epoch 5/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - loss: 0.2222 - mae: 0.5068 - val_loss: 0.2192 - val_mae: 0.5020\n",
      "Epoch 6/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 0.2211 - mae: 0.5043 - val_loss: 0.2193 - val_mae: 0.4986\n",
      "Epoch 7/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 0.2202 - mae: 0.5026 - val_loss: 0.2191 - val_mae: 0.4980\n",
      "Epoch 8/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - loss: 0.2198 - mae: 0.5015 - val_loss: 0.2192 - val_mae: 0.5010\n",
      "Epoch 9/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 44ms/step - loss: 0.2195 - mae: 0.5008 - val_loss: 0.2203 - val_mae: 0.5002\n",
      "Epoch 10/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - loss: 0.2192 - mae: 0.5004 - val_loss: 0.2192 - val_mae: 0.4996\n",
      "Example 3: Direct vs Rolling\n",
      "Series: SKU_0051 @ LOC_02\n",
      "MAE Direct : 4.27\n",
      "MAE Rolling: 4.20\n",
      "First 5 days (true / direct / rolling):\n",
      "  D+01:   23.28 /   24.91 /   24.91\n",
      "  D+02:   22.18 /   29.85 /   28.58\n",
      "  D+03:   25.65 /   33.25 /   28.30\n",
      "  D+04:   35.70 /   33.61 /   27.63\n",
      "  D+05:   30.49 /   30.76 /   27.00\n",
      "------------------------------------------------------------\n",
      "Example 4: Forecast -> Replenishment\n",
      "Series: SKU_0027 @ LOC_00\n",
      "On hand: 112.7 | On order: 99.5 | Lead time: 7 days\n",
      "Lead-time demand: 227.88\n",
      "Safety stock:     18.45 (service level=0.95)\n",
      "Reorder point:    246.33\n",
      "Inv position:     212.25\n",
      "Order suggested:  34.07\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. BUILT-IN CHECKS / DEMO RUNNER\n",
    "# =============================================================================\n",
    "\n",
    "def main() -> None:\n",
    "    # Example 1: minimal shapes sanity check\n",
    "    example_1_minimal_seq2seq_shapes()\n",
    "\n",
    "    # Example 2: train on simulated panel (this is the \"core\" lesson)\n",
    "    model, df_scaled, stats, sku_map, loc_map = example_2_train_on_simulated_panel()\n",
    "\n",
    "    # Example 3: compare recursive vs direct forecasting\n",
    "    example_3_compare_direct_vs_recursive(model, df_scaled, stats, sku_map, loc_map)\n",
    "\n",
    "    # Example 4: show replenishment decision from forecast curve\n",
    "    example_4_forecast_to_replenishment(model, df_scaled, stats, sku_map, loc_map)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7305ab92-dfd2-40e8-b4d0-9c81e13ccec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
