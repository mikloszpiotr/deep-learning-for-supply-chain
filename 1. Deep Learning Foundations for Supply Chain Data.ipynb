{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303411a7-993b-47ee-bc1d-1d107ba64800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# File: dl_foundations_scm_data.py\n",
    "# Purpose: Show how supply-chain data differs from standard ML data and why DL helps\n",
    "# Topic: Multi-entity + time dependencies (SKU–Location–Customer) with lag effects\n",
    "# Input: Synthetic order history (date, sku_id, location_id, customer_id, demand)\n",
    "# Output: Baseline ML vs Deep Learning models + simple evaluation (MAPE)\n",
    "# =============================================================================\n",
    "\n",
    "# ==== 0. Imports ==============================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5a11d9-8ef7-44e4-932b-e20608ac2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1. Top-level config & constants ========================================\n",
    "SEED = 42\n",
    "N_DAYS = 240\n",
    "N_SKUS = 40\n",
    "N_LOCATIONS = 12\n",
    "N_CUSTOMERS = 30\n",
    "\n",
    "# Time-series feature engineering\n",
    "LAGS = [1, 7, 14]            # Lag effects (yesterday, last week, 2 weeks)\n",
    "ROLLING_WINDOWS = [7, 28]    # Demand smoothing windows\n",
    "\n",
    "# Modeling\n",
    "TRAIN_RATIO = 0.75\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Sequence model config\n",
    "SEQ_LEN = 28  # how much history we feed to the DL sequence model\n",
    "\n",
    "# Columns (keep naming consistent across files in your course)\n",
    "COL_DATE = \"date\"\n",
    "COL_SKU = \"sku_id\"\n",
    "COL_LOC = \"location_id\"\n",
    "COL_CUST = \"customer_id\"\n",
    "COL_DEMAND = \"demand\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7baa123a-bac8-489f-afa5-e3abf9b5d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 2. Reproducibility helpers =============================================\n",
    "def set_seeds(seed: int = SEED) -> None:\n",
    "    \"\"\"Set seeds for reproducible runs.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56de4ac2-b417-4f0b-bf6b-2b845df4ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 3. Data generation (SCM-like structure) ================================\n",
    "def generate_scm_orders(\n",
    "    n_days: int,\n",
    "    n_skus: int,\n",
    "    n_locations: int,\n",
    "    n_customers: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a synthetic dataset that mimics supply-chain demand drivers:\n",
    "    - Multi-entity hierarchy: SKU x Location x Customer\n",
    "    - Calendar seasonality + weekly patterns\n",
    "    - Promotions/events (semi-structured \"event-like\" shocks)\n",
    "    - Lead/lag effects (demand depends on past demand)\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(\"2024-01-01\", periods=n_days, freq=\"D\")\n",
    "\n",
    "    # Build the Cartesian product: each day has many entity combinations.\n",
    "    # This is common in SCM: millions of rows across SKU–Location–Customer–Day.\n",
    "    grid = pd.MultiIndex.from_product(\n",
    "        [dates, range(n_skus), range(n_locations), range(n_customers)],\n",
    "        names=[COL_DATE, COL_SKU, COL_LOC, COL_CUST],\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    # Entity “strengths” (nonlinear + heterogenous effects)\n",
    "    sku_base = np.random.lognormal(mean=2.0, sigma=0.4, size=n_skus)\n",
    "    loc_factor = np.random.uniform(0.7, 1.3, size=n_locations)\n",
    "    cust_factor = np.random.uniform(0.6, 1.5, size=n_customers)\n",
    "\n",
    "    # Calendar patterns\n",
    "    dow = grid[COL_DATE].dt.dayofweek.values  # 0..6\n",
    "    week_seasonality = 1.0 + 0.15 * np.sin(2 * np.pi * dow / 7.0)\n",
    "\n",
    "    day_idx = (grid[COL_DATE] - grid[COL_DATE].min()).dt.days.values\n",
    "    year_seasonality = 1.0 + 0.20 * np.sin(2 * np.pi * day_idx / 90.0)  # ~quarterly-ish\n",
    "\n",
    "    # Event shocks (semi-structured \"events\"): sparse, bursty, non-linear impact\n",
    "    # Why it matters: SCM often has irregular events (promotions, disruptions).\n",
    "    event_flag = (np.random.rand(len(grid)) < 0.01).astype(np.float32)\n",
    "    event_lift = 1.0 + event_flag * np.random.uniform(0.5, 2.0, size=len(grid))\n",
    "\n",
    "    # Compose demand mean (before lag)\n",
    "    base_mean = (\n",
    "        sku_base[grid[COL_SKU].values]\n",
    "        * loc_factor[grid[COL_LOC].values]\n",
    "        * cust_factor[grid[COL_CUST].values]\n",
    "        * week_seasonality\n",
    "        * year_seasonality\n",
    "        * event_lift\n",
    "    )\n",
    "\n",
    "    # Add stochasticity and clipping at 0 (demand cannot be negative)\n",
    "    noise = np.random.normal(loc=0.0, scale=0.25, size=len(grid))\n",
    "    demand = np.maximum(0.0, base_mean * (1.0 + noise))\n",
    "\n",
    "    df = grid.copy()\n",
    "    df[COL_DEMAND] = demand.astype(np.float32)\n",
    "    df[\"event_flag\"] = event_flag\n",
    "\n",
    "    # Add a realistic “missingness” pattern: not every SKU sells daily in every node.\n",
    "    # This is a key difference vs many “clean” ML datasets.\n",
    "    mask = np.random.rand(len(df)) > 0.10  # 10% missing rows\n",
    "    df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19acbafb-91e1-4114-b27c-6956be3ed7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 4. Feature engineering (lags + rolling stats) ==========================\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add calendar features useful in SCM (dow, month, etc.).\"\"\"\n",
    "    out = df.copy()\n",
    "    out[\"dow\"] = out[COL_DATE].dt.dayofweek.astype(np.int16)\n",
    "    out[\"month\"] = out[COL_DATE].dt.month.astype(np.int16)\n",
    "    out[\"day_idx\"] = (out[COL_DATE] - out[COL_DATE].min()).dt.days.astype(np.int32)\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_lag_rolling_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add lag and rolling features per (SKU, Location, Customer).\n",
    "    Why: SCM demand has temporal dependencies and lag effects (ordering cycles,\n",
    "    planning calendars, promotions, replenishment patterns).\n",
    "    \"\"\"\n",
    "    out = df.sort_values([COL_SKU, COL_LOC, COL_CUST, COL_DATE]).copy()\n",
    "    grp = out.groupby([COL_SKU, COL_LOC, COL_CUST], sort=False)[COL_DEMAND]\n",
    "\n",
    "    for lag in LAGS:\n",
    "        out[f\"lag_{lag}\"] = grp.shift(lag)\n",
    "\n",
    "    for w in ROLLING_WINDOWS:\n",
    "        # Use past-only windows to avoid leakage.\n",
    "        out[f\"roll_mean_{w}\"] = grp.shift(1).rolling(w).mean()\n",
    "\n",
    "    # A common SCM trick: treat missing history as 0 for intermittent demand.\n",
    "    # Not always correct, but helpful as a starting educational baseline.\n",
    "    lag_cols = [f\"lag_{l}\" for l in LAGS] + [f\"roll_mean_{w}\" for w in ROLLING_WINDOWS]\n",
    "    out[lag_cols] = out[lag_cols].fillna(0.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_test_split_time(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Time-based split to prevent future leakage.\"\"\"\n",
    "    df_sorted = df.sort_values(COL_DATE).copy()\n",
    "    cutoff_idx = int(len(df_sorted) * TRAIN_RATIO)\n",
    "    train = df_sorted.iloc[:cutoff_idx].copy()\n",
    "    test = df_sorted.iloc[cutoff_idx:].copy()\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbfea05d-cc1f-4f9e-a1d3-4421f8f4cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 5. Metrics ==============================================================\n",
    "def mape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-6) -> float:\n",
    "    \"\"\"Mean Absolute Percentage Error with epsilon to avoid div-by-zero.\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / denom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41bb6431-c6b1-4194-9b1d-42208a8a0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 6. Example 1: Baseline \"classical ML\" (linear regression) ===============\n",
    "def fit_linear_baseline(train: pd.DataFrame, test: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    A simple baseline:\n",
    "    - Use only engineered numeric features (lags/rolling/calendar)\n",
    "    - Ignore high-cardinality categorical interactions (SKU/LOC/CUST)\n",
    "\n",
    "    Why it often fails in SCM at scale:\n",
    "    - Non-linear effects are common (promos, substitution, thresholds)\n",
    "    - High-cardinality entity interactions explode feature space\n",
    "    \"\"\"\n",
    "    # Minimal, numeric feature set\n",
    "    features = [\"dow\", \"month\", \"day_idx\", \"event_flag\"] + \\\n",
    "               [f\"lag_{l}\" for l in LAGS] + [f\"roll_mean_{w}\" for w in ROLLING_WINDOWS]\n",
    "\n",
    "    X_tr = train[features].to_numpy(dtype=np.float32)\n",
    "    y_tr = train[COL_DEMAND].to_numpy(dtype=np.float32)\n",
    "    X_te = test[features].to_numpy(dtype=np.float32)\n",
    "    y_te = test[COL_DEMAND].to_numpy(dtype=np.float32)\n",
    "\n",
    "    # Closed-form linear regression (least squares)\n",
    "    # Add bias term\n",
    "    X_tr_b = np.hstack([np.ones((len(X_tr), 1), dtype=np.float32), X_tr])\n",
    "    X_te_b = np.hstack([np.ones((len(X_te), 1), dtype=np.float32), X_te])\n",
    "\n",
    "    # Solve: beta = (X'X)^(-1) X'y\n",
    "    beta, *_ = np.linalg.lstsq(X_tr_b, y_tr, rcond=None)\n",
    "    preds = X_te_b @ beta\n",
    "\n",
    "    return {\n",
    "        \"mape\": mape(y_te, preds),\n",
    "        \"y_mean_test\": float(np.mean(y_te)),\n",
    "        \"pred_mean_test\": float(np.mean(preds)),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2162e0d-b52b-4c31-af41-7a2477b8d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 7. Example 2: DL on tabular SCM data (embeddings + MLP) =================\n",
    "@dataclass\n",
    "class TabularDlArtifacts:\n",
    "    model: tf.keras.Model\n",
    "    feature_means: np.ndarray\n",
    "    feature_stds: np.ndarray\n",
    "\n",
    "\n",
    "def build_tabular_dl_model(\n",
    "    n_skus: int,\n",
    "    n_locations: int,\n",
    "    n_customers: int,\n",
    "    n_num_features: int,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Embeddings for SKU/Location/Customer + numeric features.\n",
    "    Why this helps in SCM:\n",
    "    - Learns dense representations for high-cardinality entities\n",
    "    - Captures non-linear interactions without manual one-hot explosion\n",
    "    \"\"\"\n",
    "    # Categorical inputs (integer ids)\n",
    "    sku_in = tf.keras.Input(shape=(), dtype=tf.int32, name=COL_SKU)\n",
    "    loc_in = tf.keras.Input(shape=(), dtype=tf.int32, name=COL_LOC)\n",
    "    cust_in = tf.keras.Input(shape=(), dtype=tf.int32, name=COL_CUST)\n",
    "\n",
    "    # Numeric input\n",
    "    num_in = tf.keras.Input(shape=(n_num_features,), dtype=tf.float32, name=\"num_features\")\n",
    "\n",
    "    # Embeddings (dimensions are small on purpose for tutorial clarity)\n",
    "    sku_emb = tf.keras.layers.Embedding(n_skus, 8, name=\"sku_emb\")(sku_in)\n",
    "    loc_emb = tf.keras.layers.Embedding(n_locations, 4, name=\"loc_emb\")(loc_in)\n",
    "    cust_emb = tf.keras.layers.Embedding(n_customers, 6, name=\"cust_emb\")(cust_in)\n",
    "\n",
    "    # Flatten embeddings\n",
    "    sku_vec = tf.keras.layers.Flatten()(sku_emb)\n",
    "    loc_vec = tf.keras.layers.Flatten()(loc_emb)\n",
    "    cust_vec = tf.keras.layers.Flatten()(cust_emb)\n",
    "\n",
    "    # Combine\n",
    "    x = tf.keras.layers.Concatenate()([sku_vec, loc_vec, cust_vec, num_in])\n",
    "\n",
    "    # Non-linear layers\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "    # Positive demand: use softplus to avoid negative predictions\n",
    "    out = tf.keras.layers.Dense(1, activation=\"softplus\", name=\"demand_pred\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[sku_in, loc_in, cust_in, num_in], outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=\"mae\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def standardize_features(\n",
    "    train_num: np.ndarray,\n",
    "    test_num: np.ndarray,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Standardize numeric features using train statistics to avoid leakage.\"\"\"\n",
    "    means = train_num.mean(axis=0, keepdims=True)\n",
    "    stds = train_num.std(axis=0, keepdims=True) + 1e-6\n",
    "    return (train_num - means) / stds, (test_num - means) / stds, means.squeeze(), stds.squeeze()\n",
    "\n",
    "\n",
    "def fit_tabular_dl(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[TabularDlArtifacts, Dict[str, float]]:\n",
    "    \"\"\"Train DL model and return artifacts + evaluation metrics.\"\"\"\n",
    "    num_cols = [\"dow\", \"month\", \"day_idx\", \"event_flag\"] + \\\n",
    "               [f\"lag_{l}\" for l in LAGS] + [f\"roll_mean_{w}\" for w in ROLLING_WINDOWS]\n",
    "\n",
    "    tr_num = train[num_cols].to_numpy(dtype=np.float32)\n",
    "    te_num = test[num_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "    tr_num_s, te_num_s, means, stds = standardize_features(tr_num, te_num)\n",
    "\n",
    "    y_tr = train[COL_DEMAND].to_numpy(dtype=np.float32)\n",
    "    y_te = test[COL_DEMAND].to_numpy(dtype=np.float32)\n",
    "\n",
    "    model = build_tabular_dl_model(\n",
    "        n_skus=N_SKUS,\n",
    "        n_locations=N_LOCATIONS,\n",
    "        n_customers=N_CUSTOMERS,\n",
    "        n_num_features=tr_num_s.shape[1],\n",
    "    )\n",
    "\n",
    "    # Build tf.data for efficiency\n",
    "    ds_tr = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "                COL_SKU: train[COL_SKU].to_numpy(np.int32),\n",
    "                COL_LOC: train[COL_LOC].to_numpy(np.int32),\n",
    "                COL_CUST: train[COL_CUST].to_numpy(np.int32),\n",
    "                \"num_features\": tr_num_s,\n",
    "            },\n",
    "            y_tr,\n",
    "        )\n",
    "    ).shuffle(50_000, seed=SEED).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    ds_te = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "                COL_SKU: test[COL_SKU].to_numpy(np.int32),\n",
    "                COL_LOC: test[COL_LOC].to_numpy(np.int32),\n",
    "                COL_CUST: test[COL_CUST].to_numpy(np.int32),\n",
    "                \"num_features\": te_num_s,\n",
    "            },\n",
    "            y_te,\n",
    "        )\n",
    "    ).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    model.fit(ds_tr, validation_data=ds_te, epochs=EPOCHS, verbose=0)\n",
    "\n",
    "    preds = model.predict(ds_te, verbose=0).reshape(-1)\n",
    "    return (\n",
    "        TabularDlArtifacts(model=model, feature_means=means, feature_stds=stds),\n",
    "        {\"mape\": mape(y_te, preds), \"y_mean_test\": float(np.mean(y_te)), \"pred_mean_test\": float(np.mean(preds))},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa34acd-7f53-4412-aaa9-abf18898e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 8. Example 3: DL sequence model (LSTM over demand history) ===============\n",
    "def make_sequences(df: pd.DataFrame, seq_len: int = SEQ_LEN) -> Tuple[Dict[str, np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build sequences per entity group (SKU, LOC, CUST).\n",
    "    Input: last seq_len demand values -> predict next-day demand.\n",
    "\n",
    "    Why: Many SCM signals are truly temporal, not just tabular.\n",
    "    LSTM/Temporal models can learn:\n",
    "    - repeated replenishment cycles\n",
    "    - holiday/weekday patterns\n",
    "    - intermittent demand regimes\n",
    "    \"\"\"\n",
    "    df = df.sort_values([COL_SKU, COL_LOC, COL_CUST, COL_DATE]).copy()\n",
    "    sequences = []\n",
    "    sku_ids, loc_ids, cust_ids = [], [], []\n",
    "    targets = []\n",
    "\n",
    "    for (sku, loc, cust), g in df.groupby([COL_SKU, COL_LOC, COL_CUST], sort=False):\n",
    "        y = g[COL_DEMAND].to_numpy(np.float32)\n",
    "\n",
    "        # Skip short histories\n",
    "        if len(y) <= seq_len:\n",
    "            continue\n",
    "\n",
    "        for i in range(seq_len, len(y)):\n",
    "            sequences.append(y[i - seq_len:i])\n",
    "            targets.append(y[i])\n",
    "            sku_ids.append(sku)\n",
    "            loc_ids.append(loc)\n",
    "            cust_ids.append(cust)\n",
    "\n",
    "    X_seq = np.stack(sequences, axis=0)  # (N, seq_len)\n",
    "    y = np.array(targets, dtype=np.float32)\n",
    "\n",
    "    # Add channel dimension for RNN input: (N, seq_len, 1)\n",
    "    X_seq = X_seq[..., None]\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            COL_SKU: np.array(sku_ids, dtype=np.int32),\n",
    "            COL_LOC: np.array(loc_ids, dtype=np.int32),\n",
    "            COL_CUST: np.array(cust_ids, dtype=np.int32),\n",
    "            \"demand_seq\": X_seq,\n",
    "        },\n",
    "        y,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_sequence_model(n_skus: int, n_locations: int, n_customers: int, seq_len: int) -> tf.keras.Model:\n",
    "    \"\"\"Embeddings + LSTM over demand sequences.\"\"\"\n",
    "    sku_in = tf.keras.Input(shape=(), dtype=tf.int32, name=COL_SKU)\n",
    "    loc_in = tf.keras.Input(shape=(), dtype=tf.int32, name=COL_LOC)\n",
    "    cust_in = tf.keras.Input(shape=(), dtype=tf.int32, name=COL_CUST)\n",
    "\n",
    "    seq_in = tf.keras.Input(shape=(seq_len, 1), dtype=tf.float32, name=\"demand_seq\")\n",
    "\n",
    "    sku_vec = tf.keras.layers.Flatten()(tf.keras.layers.Embedding(n_skus, 8)(sku_in))\n",
    "    loc_vec = tf.keras.layers.Flatten()(tf.keras.layers.Embedding(n_locations, 4)(loc_in))\n",
    "    cust_vec = tf.keras.layers.Flatten()(tf.keras.layers.Embedding(n_customers, 6)(cust_in))\n",
    "\n",
    "    seq_feat = tf.keras.layers.LSTM(32)(seq_in)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([sku_vec, loc_vec, cust_vec, seq_feat])\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = tf.keras.layers.Dense(1, activation=\"softplus\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[sku_in, loc_in, cust_in, seq_in], outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss=\"mae\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_sequence_dl(train: pd.DataFrame, test: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Train and evaluate the LSTM model.\"\"\"\n",
    "    X_tr, y_tr = make_sequences(train, SEQ_LEN)\n",
    "    X_te, y_te = make_sequences(test, SEQ_LEN)\n",
    "\n",
    "    # Standardize sequences (optional, but often helps):\n",
    "    # Use log1p for demand to stabilize variance (common in SCM).\n",
    "    # This is a “why” comment: demand tends to be skewed and heteroscedastic.\n",
    "    def log1p_pack(X: Dict[str, np.ndarray], y: np.ndarray) -> Tuple[Dict[str, np.ndarray], np.ndarray]:\n",
    "        X2 = dict(X)\n",
    "        X2[\"demand_seq\"] = np.log1p(X2[\"demand_seq\"])\n",
    "        y2 = np.log1p(y)\n",
    "        return X2, y2\n",
    "\n",
    "    X_tr, y_tr_l = log1p_pack(X_tr, y_tr)\n",
    "    X_te, y_te_l = log1p_pack(X_te, y_te)\n",
    "\n",
    "    model = build_sequence_model(N_SKUS, N_LOCATIONS, N_CUSTOMERS, SEQ_LEN)\n",
    "\n",
    "    ds_tr = tf.data.Dataset.from_tensor_slices((X_tr, y_tr_l)).shuffle(50_000, seed=SEED).batch(BATCH_SIZE)\n",
    "    ds_te = tf.data.Dataset.from_tensor_slices((X_te, y_te_l)).batch(BATCH_SIZE)\n",
    "\n",
    "    model.fit(ds_tr, validation_data=ds_te, epochs=EPOCHS, verbose=0)\n",
    "\n",
    "    # Predict back-transform\n",
    "    preds_l = model.predict(ds_te, verbose=0).reshape(-1)\n",
    "    preds = np.expm1(preds_l)\n",
    "\n",
    "    return {\"mape\": mape(y_te, preds), \"y_mean_test\": float(np.mean(y_te)), \"pred_mean_test\": float(np.mean(preds))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18cba7b5-6696-445b-9fdd-e48079e1483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 9. When deep learning is justified in SCM ===============================\n",
    "def deep_learning_justification_checklist() -> None:\n",
    "    \"\"\"\n",
    "    Practical rule-of-thumb checklist (printed as guidance).\n",
    "    Keep this in code so learners see it near the implementation context.\n",
    "    \"\"\"\n",
    "    print(\"\\nDL is usually justified in SCM when you have at least one of:\")\n",
    "    print(\"  - High-cardinality entities (thousands+ SKUs/locations/customers)\")\n",
    "    print(\"  - Non-linear drivers (promos, substitution, constraints, regime shifts)\")\n",
    "    print(\"  - Strong temporal structure (lags, cycles, intermittent demand)\")\n",
    "    print(\"  - Multi-modal data (text events, sensor streams, images)\")\n",
    "    print(\"  - Scale where feature engineering / one-hot becomes impractical\")\n",
    "    print(\"\\nClassical ML may be enough when:\")\n",
    "    print(\"  - Few entities + stable patterns + clean engineered features\")\n",
    "    print(\"  - Data volume is limited, or interpretability is primary requirement\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75aaa96e-0c59-4629-b60d-7b34a5697a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 - Linear baseline metrics: {'mape': 174.2105255126953, 'y_mean_test': 8.37143611907959, 'pred_mean_test': 7.556517601013184}\n",
      "Example 2 - Tabular DL (embeddings + MLP) metrics: {'mape': 197.01075744628906, 'y_mean_test': 8.37143611907959, 'pred_mean_test': 7.959930896759033}\n",
      "Example 3 - Sequence DL (LSTM) metrics: {'mape': 140.90504455566406, 'y_mean_test': 7.829433441162109, 'pred_mean_test': 7.357210636138916}\n",
      "\n",
      "DL is usually justified in SCM when you have at least one of:\n",
      "  - High-cardinality entities (thousands+ SKUs/locations/customers)\n",
      "  - Non-linear drivers (promos, substitution, constraints, regime shifts)\n",
      "  - Strong temporal structure (lags, cycles, intermittent demand)\n",
      "  - Multi-modal data (text events, sensor streams, images)\n",
      "  - Scale where feature engineering / one-hot becomes impractical\n",
      "\n",
      "Classical ML may be enough when:\n",
      "  - Few entities + stable patterns + clean engineered features\n",
      "  - Data volume is limited, or interpretability is primary requirement\n",
      "\n",
      "Sanity check: script finished successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==== 10. Built-in checks / demo =============================================\n",
    "def run_demo() -> None:\n",
    "    set_seeds(SEED)\n",
    "\n",
    "    # ---- Load (generate) data\n",
    "    df = generate_scm_orders(N_DAYS, N_SKUS, N_LOCATIONS, N_CUSTOMERS)\n",
    "    df = add_time_features(df)\n",
    "    df = add_lag_rolling_features(df)\n",
    "\n",
    "    train, test = train_test_split_time(df)\n",
    "\n",
    "    # ---- Example 1: Linear baseline\n",
    "    lin_metrics = fit_linear_baseline(train, test)\n",
    "    print(\"Example 1 - Linear baseline metrics:\", lin_metrics)\n",
    "\n",
    "    # ---- Example 2: Tabular DL with embeddings\n",
    "    _, tab_metrics = fit_tabular_dl(train, test)\n",
    "    print(\"Example 2 - Tabular DL (embeddings + MLP) metrics:\", tab_metrics)\n",
    "\n",
    "    # ---- Example 3: Sequence DL (LSTM)\n",
    "    # Note: This is heavier; with large grids it can still be big.\n",
    "    seq_metrics = fit_sequence_dl(train, test)\n",
    "    print(\"Example 3 - Sequence DL (LSTM) metrics:\", seq_metrics)\n",
    "\n",
    "    deep_learning_justification_checklist()\n",
    "\n",
    "    # ---- Tiny “try yourself” tasks\n",
    "    # TODO 1: Increase SEQ_LEN to 56 and compare MAPE. Does longer history help?\n",
    "    # TODO 2: Add a \"price_index\" synthetic feature and see if DL benefits more than linear.\n",
    "    # TODO 3: Change missingness rate in generate_scm_orders() and observe performance impact.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo()\n",
    "\n",
    "    # ---- Optional: quick sanity print for learners\n",
    "    print(\"\\nSanity check: script finished successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a974b28-da60-48b7-aba9-56a6a6a0ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Machine Learning vs Deep Learning in Supply Chain – Detailed Summary\n",
    "\n",
    "**Typical problems and data**  \n",
    "- In supply chains, Machine Learning (ML) is widely used on structured, tabular data for demand forecasting, inventory optimization, lead‑time prediction, and routing decisions, often using transactional ERP, POS, and logistics data.[web:60][web:61][web:66]  \n",
    "- Deep Learning (DL) is increasingly applied when supply chains involve large volumes of complex or unstructured data, such as sensor streams from IoT devices, shipment event logs, satellite imagery, scanner images, documents, and text signals from news or social media.[web:56][web:59][web:63][web:65]  \n",
    "\n",
    "**Model types typically used**  \n",
    "- ML in supply chain commonly uses gradient boosting, random forests, regularized regression, and classical time‑series models for tasks like demand forecasting, safety stock optimization, supplier risk scoring, and delivery‑time prediction.[web:60][web:61][web:62][web:70]  \n",
    "- DL uses architectures such as recurrent networks (LSTM/GRU), CNNs, and transformers to capture long‑range temporal dependencies in demand, complex spatial–temporal patterns in logistics networks, and multimodal signals (text, images, sensor data) affecting supply chain performance.[web:56][web:63][web:64][web:65]  \n",
    "\n",
    "**Feature engineering vs representation learning**  \n",
    "- In ML‑based supply chain projects, domain experts craft features like promotions flags, holiday calendars, price indices, stockout indicators, lead‑time buckets, and route distance metrics; model quality heavily depends on this manual feature engineering.[web:60][web:61][web:62]  \n",
    "- DL models can ingest richer raw inputs (e.g. raw time series of orders, event logs, weather grids, shipment status sequences) and automatically learn hierarchical features, which helps capture non‑linear interactions and latent patterns that manual feature sets may miss.[web:56][web:63][web:64][web:65]  \n",
    "\n",
    "**Demand forecasting examples**  \n",
    "- ML improves classical demand forecasting by using tree‑based ensembles or boosting models on engineered features to reduce MAPE and RMSE, enabling better safety stock sizing and reducing stockouts and overstock.[web:60][web:61][web:62][web:65][web:70]  \n",
    "- DL frameworks that combine SOMs, PCA, and deep neural networks, or sequence models like LSTMs and transformers, can further enhance demand and shipment‑time forecasts by modeling complex temporal patterns, regime shifts, and interactions across products, locations, and channels.[web:56][web:63][web:64][web:67]  \n",
    "\n",
    "**Logistics, routing, and risk**  \n",
    "- ML is used for ETA prediction, route‑choice modeling, anomaly detection on transport costs, and classification of shipments by delay risk using historical trip data, traffic observations, and basic external signals.[web:60][web:61][web:65]  \n",
    "- DL extends this by learning from continuous GPS traces, traffic sensor streams, camera images, and text updates, enabling more accurate dynamic routing, early delay detection, and predictive maintenance for fleet and equipment.[web:56][web:59][web:63][web:64]  \n",
    "\n",
    "**Warehouse and inventory operations**  \n",
    "- ML supports smart inventory management by predicting fast/slow movers, optimizing reorder policies, and detecting anomalies in picking times or inventory records, generally from structured inventory and operations data.[web:60][web:66][web:69]  \n",
    "- DL powers advanced warehouse automation such as vision‑based counting and damage detection, robot navigation, automatic pallet recognition, and complex multi‑step decision policies learned from sensor and camera data.[web:56][web:59][web:60][web:64]  \n",
    "\n",
    "**Data volume, infrastructure, and deployment**  \n",
    "- ML solutions in supply chain often work well with historical datasets at the scale of thousands or millions of rows and can be deployed within existing planning tools or cloud platforms with moderate compute resources.[web:58][web:60][web:61]  \n",
    "- DL solutions usually require higher data volumes from multiple systems (IoT, WMS/TMS, external feeds), GPU‑accelerated infrastructure, and more complex MLOps practices, but can yield significant gains in forecasting accuracy, routing efficiency, and automation quality when these prerequisites are met.[web:56][web:63][web:64][web:65]  \n",
    "\n",
    "**Business trade‑offs and when to use which**  \n",
    "- For many organizations, ML is a strong first step to modernizing planning and execution, offering interpretable models that integrate relatively easily with existing ERP, APS, and BI stacks for decisions like inventory targets, replenishment, and capacity planning.[web:58][web:60][web:61][web:69]  \n",
    "- DL becomes attractive when supply chains generate diverse, high‑frequency data and the value of incremental accuracy or automation is high, such as in global e‑commerce networks, real‑time logistics platforms, or highly automated warehouses, where more complex models can materially improve service levels and cost.[web:56][web:63][web:64][web:65]  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
